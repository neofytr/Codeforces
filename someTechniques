1. In questions that have the count of somethings changing when an operation is done and the total number of somethings remain constant, check how the parity of the count of things changes after an operation

2. In questions in which we can perform any operation from a group of operations, one after other, there is usually an optimal order for the
application of these operations (i.e., something like all operation 1s must occur before any operation 2s and so on) (see random/gettingZero)

3. For any x > 1, __builtin_ctzll(x) is the power of 2 in the prime factorization of x

4. If n is odd and >= 3, then
n, n - 1 and n - 2 have no prime
factors in common and
LCM(n, n - 1, n - 2) = n * (n - 1) * (n - 2)

5. For n >= 2, gcd(n, n - 1) = 1

6. Let x >= 1
Then, there can be atmost 1 number in the sequence

x, x + 1, x + 2, ..., x + (r - 1)

for some r >= 1 that is divisible by a number >= r

7. Count/Find triplets 1 <= i < j < k <= n that satisfy some conditions in O(n^2)

for (int k = 2; k <= n - 1; k++) {
	// iterate through [1, k - 1] to choose a suitable i

	// iterate through [k + 1, n] to choose a suitable j

	// check if (i, j, k) is a suitable pair
}

Thus, if we have to find a triplet, fix the middle one and solve two independent problems (if possible)

8. Let n >= 2
Let a1, a2, ..., an is a sequence of integers

If a1 <= an, then there are
two consecutive elements x, y
of the sequence such that x <= y

9. Let p(x1, y1) and q(x2, y2) be two points on a line y = mx + c.

Then, distance(p, q) = |x2 - x1| * root(1 + m * m)

For any integral m != 0, distance(p, q) is always irrational

10. For two distinct points P and Q in a plane to not have integral distance between them, it's necessary for them to be have different rows or different columns

11. For constructive questions, say we have to construct the maximal set satisfying some conditions, we can adopt the following approach:

i. Get an upper bound on the size of the set
ii. Do something to construct a set of that size

12. For x >= 0

i. floor(x / 2) = x >> 1
ii. ceil(x / 2) = (x >> 1) + (x & 1)

13.

Consider a sorted array v[1, n]

Let f(r) = |r - v[1]| + |r + 1 - v[2]| + ... |r + n - 1 - v[n]|

         = |v[1] - (r + 0)| + ... + |v[n] - (r + (n - 1))|

We can calculate this in O(log(n)) with O(n) precomputation as follows:

1. Construct an array s[1, n] such that

s[i] = v[i] - (i - 1) for 1 <= i <= n

3. Construct the prefix array of s, say p[0, n]

p[0] = 0 and p[r] = p[r - 1] + s[r] for 1 <= r <= n

2. Find the first index k in the array v[1, n] such that v[k] >= r

If such a k does not exist, then
f(r) = r * n - p[n]

If k == 1, then f(r) = p[n] - r * n

Otherwise,

f(r) = (p[n] - p[k - 1]) - r * (n - k + 1) + r * (k - 1) - (p[k - 1])

14.

Let n >= 1
Let S = {1, 2, ..., n}
Let A = {r is a natural number such that 1 <= r <= n * (n + 1) / 2}

Then, for any a in A, there is a subset s of S such that sum(s) = a
Also, there is no subset s of S such that
sum(s) > n * (n + 1) / 2

15.

Let n >= 3
Let l1, l2, ... , ln be positive integers

A polygon with n sides can be formed with
these integers as its side lengths iff

2 * max(l1, l2, ..., ln) <= l1 + l2 + ... + ln

, i.e, no side-length is greater than all the other side-lengths combined

16. In a problem where we are to count
the number of pairs of indexes (i, j) where 1 <= i < j <= n in an array arr[1, n] satisfying some condition, we can permute (usually, the permutation of the array used is the sorted one) the array beforehand and solve the problem if

i. Condition depends on values, not positions
ii. The number of pairs is invariant under the permutation
iii. Counting pairs/combinations/subsets
iv. Answer is unchanged by reordering (permutation invariant)

We cannot permute the array if

i. Problem involves subarrays/prefixes/suffixes
ii. Ordering of the elements matters
iii. Distance/position constraints are essential

16.

Let k >= 0

Let A[1, n] and B[1, m] be two integer arrays such that

0 <= A[r] < 2^k for 1 <= r <= n

Let f[s] = Summation from r = 1 to n of (A[r] ^ B[s]) for 1 <= s <= m

Suppose we want to find max(f[s] for 1 <= s <= m) in O(n + m)

We can do this as follows:

vector<int> bit(k);
for (int r = 1; r <= n; r++)
	for (int b = 0; b <= k - 1; b++)
		if (A[r] & (1 << b)) bit[b]++;

// We can now calculate each f[s] for 1 <= s <= m in O(1) now after this precomputation that is done in O(n)

int maxi = LLONG_MIN;
for (int r = 1; r <= m; r++) {
	int sum = 0;
	for (int b = 0; b <= k - 1; b++)
		if (B[r] & (1 << b)) sum += bit[b] * (1 << b);
		else sum += (n - bit[b]) * (1 << b)
}

17.

Let x >= 1
Let z is the smallest power of two >= x

Then x ^ y, for y >= 1 and y != x, divides y implies that y <= z - 1

18.

Let A[1, n] and B[1, k] are sorted
array of integers

Let f: A -> B

Suppose we are to find an assignment f such that

1. summation |A[r] - B[f[r]]| for 1 <= r <= n is minimized

or

2. max(|A[r] - B[f[r]]| for 1 <= r <= n) is minimized

Then, it is not optimal to assign in a crossed manner, i.e,

If we assign B[j] to some A[r], then, it is not optimal to assign A[r + 1] to any B[s] for s < j

19. Chicken McNugget Theorem

If you have two positive integers a and b that are coprime (that is, gcd(a, b) = 1), then, the largest
integer that cannot be written in the form ax + by, where x and y are non-negative integers, is ab - a - b
After this number, every larger number is representable

20.

Let f(x) = 11...1 (total of x ones)
Then, for any x >= 2, f(x) = 11 * a + 111 * b for some non-negative integers a and b

Rough Proof:

There are three possible cases for x:

1. x = 3 * r for some r >= 1
Then f(x) = 111 * pw(10, x - 3) + 111 * pw(10, x - 6) ... + 111 * pw(10, 0) = 111 * y for some y >= 0

2. x = 3 * r + 1 = 3 * (r - 1) + 4 for r >= 2
Then f(x) = 111 * pw(10, x - 3) + .. + 111 * pw(10, 4) + 11 * 100 + 11 = 111 * a + 11 * b for some a, b >= 0

3. x = 3 * r + 2
Then f(x) = 111 * pw(10, x - 3) + ... + 111 * pw(10, 2) + 11 * 100 = 111 * a + 11 * b for some a, b >= 0

21.

Let n >= 2
Let arr[1, n] be an array of non-negative integers

Suppose we have to partition arr into consecutive subarrays such that the XOR of all the subarrays are equal.
A partition with 1 subarray is always possible (the trivial partition {{1, 2, ..., n}})

Suppose we need to have the number of partitions >= 2
Let p[0] = 0 and p[r] = p[r - 1] ^ arr[r] for 1 <= r <= n

If p[n] = 0, then we can always partition arr[1, n] into two subarrays with equal XORs ->
{{1, ..., r}, {r + 1, ..., n}} where the XORs of both the subarrays are equal
Such two subarrays always exist since if the XOR of some numbers is 0, then its possible iff the numbers are equal
And since p[n] = arr[1] ^ ... ^ arr[n] = 0, then we will always have p[n] = x ^ x where the first x is the xor of some prefix, and the
second x is xor of the rest of the array

If p[n] > 0, then, no even length partition exists since XOR of an equal number of even things is zero
So if the array can be partitioned, it will have an odd length partition
Now, since the XORs of the partitioned subarrays are equal and the partitioned subarrays are odd in number,
the XORs of these partitioned subarrays = p[n]
So, we can check if such a partition exists by the following code:

int cnt = 0, l = 1, r = 1;
while (l <= r) {
    l = r;
    while (r <= n && (p[r] ^ p[l - 1] != p[n]) r++;
    if (r <= n && p[r] ^ p[l - 1] == p[n]) cnt++;
    l = r + 1;
}

if (cnt >= 1) return true;
else return false;

22.

When during thinking of a transition for dp[r], you find that the transition involves dependency
on values of some dp[r + l] where l > 0, then, think of push-dp or recursion to solve the problem.

23. String DP Patterns

Four things to remember:

The "prefix to prefix" pattern (comparing beginnings of both strings) is the foundation of all string DP.

1. String DP compares prefixes. dp[i][j] relates prefix of string A to prefix of string B
2. Match v/s operate. When characters match, you get a free transition.
3. Three choices become one min. Replace, delete, insert map to three neighbors.
4. Base cases are edge strings. Empty-to-something costs the length.

For example consider the problem Edit Distance.
Let A[1, n] and B[1, m] be the two strings

We define dp[i][j] is the minimum number of operations required to convert A[1, i] into B[1, j]
Here are the transitions

dp[0][0] = 0
dp[0][j] = j for all 1 <= j <= m
dp[i][0] = i for all 1 <= i <= n

dp[i][j] = dp[i - 1][j - 1] if A[i] = B[j]
         = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + 1) otherwise

The final answer is dp[n][m]

24. Resource Tracking DP
dp[i][j] is the answer after i steps using j of some resource

25. Grid DP
dp[i][j] is the answer for cell at row i, column j

26. String DP
dp[i][j] is the answer comparing first i characters of one string to first j of another

27. Ordered Inclusion-Exclusion using DP (First-violation DP pattern)

This technique applies to problems of the following type:
1. We want to count objects (paths, sequences, walks, strings, processes, etc.)
2. Objects are built step by step (or move monotonically)
3. There are forbidden states / events
4. The total state space is too large for naive DP
5. Direct inclusion-exclusion over forbidden sets is exponential and impossible

The technique relies on one crucial property: Every invalid object has a unique earliest violation
This is the entire foundation. It matters because if each bad object can be charged to exactly one first
bad event, then we can subtract bad objects once and only once. This avoid exponential inclusion-exclusion.

Let:
Object = things we want to count
States = ordered checkpoints / events / positions
State order = 0 < 1 < 2 < ... < K

For this technique to work, the following property must be satisfied: If state j > i, then we cannot encounter state j and then state i afterwards

We define dp[r] = number of objects that reach state i without violating any forbidden state < i for 1 <= i <= K
For each state i, we can compute total(i) = number of ways to reach state i ignoring all constraints

Take any invalid object that reaches state i. Because of monotonicity/ordering, it must violate constraints
for the first time at some state j < i. This j is unique, well-defined and the earliest violation.

This allows a partition of invalid objects: Invalid objects reaching i = union of objects whose first violation is at j, for all j < i
This partition is disjoint.

An object whose first violation is at j has two parts:
1. A valid object from start to j
2. An unconstrained continuation from j to i

Why the first part is valid: because j is the first violation
Therefore, the number of such objects is dp[j] * ways(j -> i)

This is the key multiplicative structure.

Putting everything together:

dp[i] = total(i)
for all j < i: dp[i] -= dp[j] * ways(j -> i)

This is the final transition.

28. Ordered IE DP
Suppose we have some states a, s1, s2, ..., sk < b that are distinct
Suppose we are to find the number of paths from state a to state b such that we don't pass through any of
the states s1, ..., sk.

This can be done using IE but that is exponential in k. This can be reduced to O(k^2) using the following technique.

This technique works if there is an ordering among the states a < s1 < ... < sk < b such that we cannot visit a state j
first and then visit some state i < j

Let 1 <= r <= k
Let dp[r] = number of paths from state a to state sr that do not pass through any state si where i < r
Now dp[r] = total_paths(a, sr) - (paths from state a to state sr that pass through some state si where i < r)

Let a path from state a to state sr be called invalid if it passes through some state si where i < r
Thus, dp[r] = total_paths(a, sr) - (number of invalid paths from state a to state sr)

Let p is an invalid path from state a to state sr. Due to the above property, the path p has a unique
transition state st for 1 <= t <= r - 1 such that p does not pass through any state sk where k < t and p
passes through state st.

Let Ak = set of all paths that have their transition state st = k
Now |Ak| = dp[k] * total_paths(st, sr)

Now, any invalid path from state a to state sr has a transition state st for 1 <= t <= r - 1. Thus
the set of all such invalid paths = Union of Ak for 1 <= k <= r - 1

Also, any Ai and Aj for i < j are disjoint since for any path p1 in Ai and p2 in Aj, p1 and p2 are different
since p1 passes through si and p2 cannot pass through si since j > i.

Thus, the number of such invalid paths = summation of |Ak| for 1 <= k <= r - 1

Thus, dp[r] = total_ways(a, sr) - summation of (dp[k] * total_ways(sk, sr)) for 1 <= k <= r - 1, for 1 <= r <= k
For dp[b], just replace sr in the above formula by b

29. Reverse Inner Loop for DP Space Optimization

This technique applies to DP problems with the following structure:
You process items one by one, and for each item you update DP states that represent a resource limit, i.e, 0/1 knapsack-style DP

Canonical 2D DP formulation
Let:
i = number of items considered
s = resource used

Define dp[i][s] = best/possible using first i items with resource s
Transition: dp[i][s] = either dp[i - 1][s] or dp[i - 1][s - wi] + vi (choose max/OR/min depending on the problem)

Here, dp[i][*] depends only on dp[i - 1][*]

Why space optimization is possible?
Since row i depends only on row i-1, storing the entire table is wasteful

Space reduction options:
1. Use two arrays (prev, curr)
2. Use one array and overwrite carefully

This technique focuses on the second option

If you write dp[s] = max(dp[s], dp[s - w) you are:
1. reading from dp
2. writing to dp

So you must guarantee that dp[s - w] still represents the previous item, not the current one
If this is violated -> same item reused many times

Why forward iteration fails?
Forward Loop (WRONG for 0/1 problems)

for (int s = w; s <= S; s++)
    dp[s] = max(dp[s], dp[s - w])

What happens?
1. dp[s - w] may have already been updated using the current item
2. This allows multiple uses of the same item

Result: You accidentally implement unbounded knapsack

The reverse loop solution (key idea)
for (int s = S; s >= w; s--)
    dp[s] = max(dp[s], dp[s - w])

Since w >= 1, s - w <= s and thus uses results from the last item stored in [1, s - 1], which aren't
altered when we calculate dp[s] when iterating backwards. This is why the reverse loop works and is correct.

Why this works:
When iterating backwards:
1. s - w < s
2. Since we are going from high -> low, dp[s - w] is untouched in this iteration
3. So it still equals dp[i - 1][s - w]

General Structure

0/1 Knapsack DP template (1D)

initialize dp[0, S]
for each item (wt w, value v):
    for s = S down to w:
        dp[s] = combine(dp[s], dp[s - w], v)

Where combine = max, min, or, +, etc

Multidimensional Generalization:

If DP is: dp[i][a][b], you may compress the dimension i, but all resource dimensions must be iterated backwards
for a = A down to wa:
    for b = B down to wb:
        dp[a][b] = ...

If you compress a DP dimension and each choice is allowed only once, iterate resource dimensions backwards.
In 1D optimization, we iterate capacity in reverse to ensure each item is used at most once. Forward iteration would allow the same item to be picked multiple times in a single row.

30. Value-Based Knapsack

When one dimension of DP is too large, check if you can swap it with the answer.
Standard Knapsack: capacity as index, value as answer
Value-based Knapsack: value as index, weight as answer (then filer by capacity)

This swap works when the "large" dimension is what you'd normally index by, and the "small" dimension
(total value) can become the new index. Look at the constraints carefully; they often hint at which formulation
to use

31. If the dp transition looks something like:

initialize dp[0][0]
initialize dp[0][1...W]
initialize dp[1...n][0]
for (int r = 1; r <= n; r++)
    for (int v = 1; v <= W; v++) {
        dp[r][v] = dp[r - 1][v];
        if (v >= v[r]) dp[r][v] = combine(dp[r - 1][v], dp[r - 1][v - v[r]]);
    }

where v[r] >= 1 for 1 <= r <= n, then, it can be space-optimized to a 1D DP as:

initialize dp[0] (corresponds to previous dp[0][0])
initialize dp[1...W] (corresponds to previous dp[0][1..W])
for (int r = 1; r <= n; r++)
    for (int v = W; v >= v[r]; v--)
        dp[v] = combine(dp[v], dp[v - v[r]])

Since v[r] >= 1 and we are iterating backwards, dp[v] would contain dp[r - 1][v] before the updating and dp[v - v[r]]
would contain dp[r - 1][v - v[r]] since we are iterating backwards and wouldn't have updated dp[v - v[r] yet.

32. If the DP transition looks something like:

initialize dp[0][0]
initialize dp[0][1...W]
initialize dp[1...n][0]
for (int r = 1; r <= n; r++)
    for (int v = 1; v <= W; v++) {
        dp[r][v] = dp[r][v];
        if (v >= v[r])
            dp[r][v] = combine(dp[r][v], dp[r][v - v[r]]);
    }

where v[r] >= 1 for 1 <= r <= n, then, it can be space-optimized to a 1D DP as:

initialize dp[0]        (corresponds to dp[0][0])
initialize dp[1...W]    (corresponds to dp[0][1...W])
for (int r = 1; r <= n; r++)
    for (int v = v[r]; v <= W; v++)
        dp[v] = combine(dp[v], dp[v - v[r]])

Since v[r] >= 1 and we iterate forwards, dp[v] would contain dp[r - 1][v] before the updating, and,
dp[v - v[r]] would be dp[r - 1][v - v[r]] since we would've already updated it (since we are iterating forward).

33. Some DP Lessons:

1. If a problem says "subset of a subset" or "choice inside a choice", both choices must be modeled together in a single DP state.
2. If inner decisions depend on outer decisions, add dimensions.
3. Each item may have multiple logical roles:
i. unused
ii. used in the outer subset
iii. used in both outer and inner subset
4. If dp[r] depends only on dp[r - 1], remove r (state compression)
5. When choices are nested and constrained, model them together, define DP precisely, respect bounds strictly, and compress whenever history is irrelevant.

34. Sliding Range Minimum

Let n, k >= 1
Suppose you have a sequence of values v[1, n]

Suppose, at each step i, starting from 1 and reaching till k, you want

min { v[j] + f(j) | L(i) <= j <= R(i)}
, where, 
1. R(i) increases by exactly 1 as i increases by 1
2. L(i) never decreases as i increases
3. f(j) is a function of j only and does not depend on i

This is the sliding window minimum problem
We solve this using a monotonic deque that maintains the following invariants during each step:

1. The deque stores all the indices which are inside the current window
2. The deque is sorted (in non-decreasing order) from front to the back according
   to the value of (v + f) at the index in the deque

This ensures that the front of the deque contains the index of the minimum of (v + f) inside the current window

deque<int> dq;
for (int i = 1; i <= k; i++) {  
    int right = R(i);
    int left = L(i);

    // insert first
    while (!dq.empty() && (v[dq.back()] + f(dq.back())) >= v[right] + f(right))
        dq.pop_back();
    dq.push_back(right);

    // delete expired elements later
    while (!dq.empty() && dq.front() < left)
        dq.pop_front();

    if (!dq.empty()) {
        int mIdx = dq.front();
        int mVal = v[mIdx] + f(mIdx); // minimum of the window at step i 
    }
}

The updation at each step works in amortized O(1), and the querying at each step works in O(1)

35. If we have some jobs j[1, n], and they have
some corresponding deadlines d[1, n], then, any optimal order of completing them will be increasing order of their deadlines.

36. Whenever a problem says “at most X”, model the DP as “exactly X” and sum at the end.

37. = 0 v/s >= 1 DP Optimization

This optimization applies in counting DPs where a variable represents a non-negative integer number of times something is used (repetitions allowed) and the transition loops over that count (e.g. for v = 0...).

Instead of enumerating all v, split into two exhaustive and mutually exclusive cases: v = 0 (do not use it -> transition from the previous layer, e.g., dp[i - 1][state]) and v >= 1 (use it once and reduce the problem -> transition from the same layer with reduced state, e.g, dp[i][state - cost]).

This works because all solutions with v >= 1 can be generated by repeatedly
addint one, so, the current DP accumulates them.

When to apply: counting ways, repetition allowed, cost changes as a function of the number of times v is used, and you see a loop over "how many times".

How to apply: replace the v loop with dp[i] = dp[i - 1] (v = 0) + dp[i] shifted by one cost (v >= 1).

If a DP loops over how many times something is used, split into 0 and ≥ 1 and reuse the DP.

Never loop over “how many times” in counting DP—split into 0 and ≥1 and reuse the DP.

38. Counting Knapsack

1. Combinations v/s Permutations is controlled by loop order. If the coins are the outer loop, then we are counting combinations; if the amount is the outer loop, then we are counting permutations.

2. Forward v/s backward inner loop depends on bounded v/s unbounded. Bounded (use each item once) needs backward. Unbounded (unlimited use) can go forward.

39. Bounded Knapsack

Problem: Given items (wi, vi, ki) and capacity W, maximize total value
with at most ki copies of each item.

Core Idea: Bounded knapsack is reduced to 0/1 knapsack using binary decomposition of counts.

Binary Decomposition Trick:

Any count k can be written as a sum of powers of two (i.e., its binary representation).

So, replace item (w, v, k) with multiple items:

(w * (1 << b), v * (1 << b))

for each set bit 0 <= b < 64 in k.

This converts bounded knapsack to 0/1 knapsack with O(sum(log(kr); 1 <= r <= n)) items.

Why it works?

Binary decomposition ensures:
1. Every number 0, ..., k is representable.
2. Each decomposed item is used at most once.
3. Total choice remains exact.

The binary decomposition is the key. Instead of k copies of each item, you create O(logk) bundles. Any count from 1 to k can be formed by combining these bundles. This makes nded knapsack nearly as fast as 0/1.

Complexity:

1. Time: O(W * sum(log(kr); 1 <= r <= n))
2. Space: O(W)

Pattern Recognition:

1. If problem says "at most k times", we have bounded knapsack
2. If k is large, think of binary splitting.
3. If k is small and many items, think of monotonic queue DP

40. Greedy before DP

If a DP involves choosing items where feasibility of choosing item r depends on previous chosen items, and there is no fixed natural order (like time, index, or position), then the order itself is part of the problem.

In such cases, first look for a greedy ordering that preserves feasibility (usually proven via an exchange argument), and only then apply DP in that order.

The greedy step does not choose items; it only fixes an order so that local DP transitions remain globally valid.

Typical signals include: constraints like "item r can only be chosen if previous total <= limit(r)", objects that add cost but have capacity, or rules where choosing later items can invalidate earlier ones.

Common orderings minimize future harm (e.g., sort by cost + capacity, deadlines, or slack). After this ordering, DP becomes straightforward and safe.

If DP validity depends on history/future, ask: "Is the order wrong?" -> Fix order greedily, then DP.

To recognize this pattern:
1. DP state depends of previous/future aggregates (weight, time, load, sum)
2. Items are symmetric initially (no given order)
3. Picking an item early can break feasibility later
4. Greedy alone fails, DP alone fails -> Greedy order + DP works

ORDER FIXES FEASIBILITY while DP OPTIMIZES VALUE

When DP feasibility depends on history and items are unordered, first prove a greedy order is WLOG using an exchange argument on two items; then DP over than order.

To justify restricting the DP to a specific order, it suffices to show that for any two items i and j (i > j) that appear consecutively in a feasible solution but violate the desired order (K(i) > K(j)), swapping them preserves feasibility and does not decrease the objective value.

By repeatedly applying such swaps, any optimal solution can be transformed into one respecting the order, hence the restriction is without loss of generality.

In order-sensitive DP, prove a greedy order is WLOG via an exchange argument: show that any inversion can be swapped without harming feasibility or value; then DP over that order is valid.

41. Linear/Near-Linear Counting of Prefix Comparisons with Bounded Transitions

We are given a sequence (string/array) and asked to count substrings (or index pairs) satisfying a condition that can be rewritten as:

p[j] > p[i] for 0 <= i < j <= N

where p[k] is a prefix derived quantity.

General Solution: Fenwick Trees/Merge Sort which is O(NlogN)

This technique reduces the time complexity to O(N * K), where K is a small constant, if the prefix transitions are bounded.

Critical Assumption (When This Works)

Assume that the prefix function satisfies p[k] - p[k - 1] belongs to [a, b] where

1. a and b are constants
2. b - a = K is small and independent of n

Under this assumption:
1. Prefix values are bounded by O(N)
2. Between consecutive positions, p only moves a small distance on the number line

Maintained State (Invariants)
We process indices in increasing order j = 1 -> N

Variables:
1. d = p[j] -> current prefix value
2. cnt[x] -> number of indices i < j with p[i] = x
3. sum = number of indices i < j such that p[i] < d
4. ans = total number of valid pairs counted so far

Core Invariant
At the start of iteration j:

sum = # {i < j : p[i] < p[j - 1]}
This invariant is the foundation of the algorithm

Updating the Invariant

Let:
1. old = p[j - 1]
2. new = p[j]
3. delta = new - old, where a <= delta <= b

We must update sum so that it becomes

sum = # {i < j : p[i] < new}

1. Case 1: Prefix Increases (delta > 0)

New prefix values that become strictly smaller than new: old, old + 1, ..., new - 1

Update:
for x = old to new - 1:
    sum += cnt[x]

2. Case 2: Prefix Decreases (delta < 0)
Prefix values that are no longer strictly smaller: new, new + 1, ..., old - 1

Update:
for x = new to old - 1
    sum -= cnt[x]

3. Case 3: Prefix Unchanged (delta = 0)
Nothing changes: sum unchanged

Counting Valid pairs
After updating sum:

# {i < j : p[i] < p[j]} = sum

So we do: ans += sum
Then register the current prefix for future steps: cnt[p[j]]++

Correctness Argument
1. Each valid pair (i, j) is counted exactly once, at iteration j.
2. The invariant guarantees that sum is always correct.
3. Only prefix values that cross the comparison boundary are adjusted.

If prefix values change only by a small bounded amount each step, the count of smaller previous prefixes can be maintained incrementally in O(1) or O(K) time per step, yielding an O(N) or O(N·K) solution.

42. DP Optimization via Modulo Decomposition + Sliding Window

We consider DP transitions of the form

dp[i][x] = OP (0 <= t <= ki) (dp[i - 1][x - t * wi] + f(i, t)) for 1 <= i <= n, 0 <= x <= S

where:

1. i -> stage/item index
2. x -> DP state (weight, sum, position)
3. wi -> fixed step size
4. ki -> upper bound on steps
5. f(i, t) -> additive cost depending only on t
6. OP is {sum, max, or min}

Naive Complexity

For each (i, x) we try all t: O(N * S * K)
This is infeasible when k is large

Structural Observation (The Core Invariant)

All predecessor states used to compute dp[i][x] are dp[i - 1][x], dp[i - 1][x - wi], dp[i - 1][x - 2 * wi], ...

These indicies:
1. form an AP
2. share the same remainder modulo wi

This enables the entire optimization.

Modulo Decomposition

Fix an item i
For each remainder i in {0, 1, ..., wi - 1} consider only the states:

x = r + t * wi for (S - r) / wi >= t >= 0

Define a reduced sequence:

A[r][t] = dp[i - 1][r + t * wi]

Substituting this into the original transition, we get:

dp[i][r + t * wi] = OP (t - ki <= j <= t) (A[r][j] + f(i, t - j)) for 1 <= i <= n, for 0 <= r < wi, for 0 <= t <= (S - r) / wi

For a fixed i and r, the DP is now 1D over t
As t increases by 1, the valid range of j shifts from [t - ki, t] to [t + 1 - ki, t + 1], i.e, a sliding window of fixed width

Hence, the reduced DP is a sliding window aggregate over a linear sequence.

We show an example:

Let the transition be:
dp[i][x] = max(dp[i - 1][x - t * wi] + t * vi, for 0 <= t <= ki) for 1 <= i <= n, 0 <= x <= S

For some 1 <= i <= n, let x = r + j * wi for some 0 <= r < wi and 0 <= j <= (S - r) / wi
Then, 

dp[i][r + j * wi] = max(dp[i - 1][r + (j - t) * wi] - (j - t) * vi + j * vi, for 0 <= t <= ki) for 1 <= i <= n, 0 <= r < wi, and 0 <= j <= (S - r) / wi

i.e,

dp[i][r + j * wi] = max(dp[i - 1][r + t * wi] - t * wi + j * vi, for max(0, j - ki) <= t <= j)) for 1 <= i <= n, 0 <= r < wi, and 0 <= j <= (S - r) / wi

for (int i = 1; i <= n; i++)
    for (int r = 0; r < wi; r++)
        for (int j = 0; j <= (S - r) / wi; j++) {
            int left = max(0, j - ki);
            int right = j;

            dp[i][r + j * wi] = LLONG_MIN;
            for (int t = left; t <= right; t++)
                dp[i][r + j * wi] = max(dp[i][r + j * wi], dp[i - 1][r + t * wi] - t * wi) + j * vi;
        }

The innermost transition is now a sliding window maximum and thus can be optimized using a deque

for (int i = 1; i <= n; i++)
    for (int r = 0; r < wi; r++) {
        deque<int> d;
        for (int j = 0; j <= (S - r) / wi; j++) {
            int left = max(0, j - ki);
            int right = j;

            while (!d.empty() && dp[i - 1][r + d.back() * wi] - d.back() * vi > dp[i - 1][r + right * wi] - right * vi)
                d.pop_back();
            d.push_back(right);

            while (!d.empty() && d.front() < left)
                d.pop_front();

            dp[i][r + j * wi] = LLONG_MIN;
            if (!d.empty()) dp[i][r + j * wi] = dp[i - 1][r + d.front() * wi] - d.front() * vi + j * vi;
        }
    }

43. In some dynamic programming problems, although the total number of elements or transitions is very large, the number of distinct element types is small enough to satisfy the constraints; in such cases, the DP can be significantly simplified by grouping identical elements and reformulating the transition as a bounded (0 / k) knapsack. 

The key observation is that if multiple elements contribute identically to the DP state—having the same weight, cost, or effect—then only their count matters, not their individual identities or order. 

By counting the frequency of each distinct type and treating each type as a single item with weight w, value v, and multiplicity k, the original DP over many items reduces to transitions of the form dp[x] = (dp[x − t * w]+t * v). 

This compression reduces the effective problem size from the total number of elements to the number of unique types and enables the use of standard bounded-knapsack optimizations such as binary decomposition, monotonic queue optimization, or modulo + sliding-window DP. 

Conceptually, this technique exploits the fact that a large input with a small “alphabet” can be processed by frequency aggregation, turning an otherwise infeasible DP into a tractable one.

44. DP via Exchange Argument

i. Problem Pattern
This technique applies when:
1. The items are initially unordered
2. The DP is order-sensitive (state depends on processed prefix)
3. Feasibility or value depends on history, not just set membership
4. Goal is to optimize or test feasibility over all permutations

ii. Core Principle
If an optimal solution can always be transformed into one that follows a specific order, then DP may be restricted to that order without loss of generality (WLOG).

The order is justified using a pairwise exchange argument.

iii. Step 1: Deriving the Order (Greedy Justification)
Define a pairwise preference rule between two items i, j
Consider two consecutive items in a feasible/optimal solution:

... -> i -> j -> ...
Suppose the desired order requires j before i

To justify this, prove:
Swapping i and j preserves feasibility and does not decrease objective value.

Formally:
If K(i) > K(j), then value(j -> i) >= value (i -> j) and feasibility is unchanged.

This establishes that j before i is safe.

iv. Step 2: Exchange Argument
1. Any solution violating the order has at least one adjacent inversion
2. Each inversion can be swapped safely
3. Repeating swaps eliminates all inversions
4. Hence, an optimal solution exists that respects the order

Therefore, restricting the DP to this order is WLOG.

v. Step 3: DP on the fixed order
Once sorted:
1. Process items sequentially
2. DP state depends only on prefix
3. No need to consider other permutations

vi. Comparator Construction Rule

The comparator encodes the pairwise exchange argument in executable form. A comparator must answer: "If I must process exactly two items back-to-back, which order is worse?"

Given two items i and j, define
1. C(i -> j): cost/feasibility effect of processing i then j
2. C(j -> i): cost/feasibility effect of processing j then i

Then, the comparator must conclude that i < j iff C(i -> j) < C(j -> i). If the cost values are equal, the order doesn't matter.

Why Two-Item Analysis is Sufficient?
Exchange arguments rely on locality:
1. Any permutation can be converted into another by adjacent swaps
2. If every local inversion is safe to swap, then global optimality follows
3. Therefore, comparing only two consecutive items is enough

This is why the comparator must be derived from a pairwise isolated scenario.

The comparator must be strict:

i < j iff C(i -> j) < C(j -> i)

Never use <=
Equal values mean order doesn't matter

The comparator must induce a strict weak ordering on the set of items. This means that the comparator defines weak order (i < j) and is transitive.

45. Bracket Sequence 

Let s is a string consisting of only the characters '(' and ')' of length n.

1. s is a correct bracket sequence iff all the '(' characters in the string can be paired with a ')' character in the string occuring after it.

2. Let f(x) = +1 if s[x] = '(' 
            = -1 otherwise

Then, s is a correct bracket sequence iff

1. summation (f(x); 1 <= x <= n) = 0
2. summation (f(x); 1 <= x <= r) >= 0 for all 1 <= r <= n

3. Let s is not a correct bracket sequence.
Let r(x) = summation(f(i); 1 <= i <= x) for 1 <= x <= n

Let g = min(r(x) for 1 <= x <= n)

Then, the optimal number of characters to be removed from s to make it a correct bracket sequence is (r(n) + 2 * |g|).

Out of these, we remove |g| number of ')' characters from s (the place of removal doesn't matter, so, possibly we remove the first |g| ')' characters), and then, remove r(n) + |g| number of '(' characters from the end of the string s.

46. Even in greedy problems where all items must be taken but the order matters, we can apply an idea similar to the “DP with sorting” technique. By comparing two items at a time—asking which order of taking them leads to a better result, just like in a DP transition—we can derive a pairwise comparator. Sorting all items using this comparator then gives an order that is globally optimal, allowing us to solve the problem greedily after this comparison-based reasoning.

46. Distance Minimizing Toolkit (1D)

1. Sum of Absolute Distances -> Median

Let n >= 1
Let a[1..n] be an array of integers

Define f(x) as:
f(x) = summation of |a[r] - x| for all r from 1 to n, where x is an integer

Let y be the median of the array a[1..n].

Then:
f(y) <= f(x) for any integer x.

If n is even, y may be chosen as any value between the two middle elements
(in particular, either of the two medians).

2. Sum of Weighted Absolute Distances -> Weighted Median

Let n >= 1
Let a[1..n] be an array of integers
Let w[1..n] be an array of positive weights

Define f(x) as:
f(x) = summation of w[r] * |a[r] - x| for all r from 1 to n, where x is an integer

Let W be the summation of all weights w[1..n].

Let y be a weighted median of a[1..n], defined as a value y such that:
    the summation of w[r] for all r with a[r] < y is at most W / 2, and
    the summation of w[r] for all r with a[r] > y is at most W / 2.

Then:
f(y) <= f(x) for any integer x.

If multiple such y exist, any of them minimizes f(x).

47. 

Let f(x) = number of consecutive ones in the binary representation of x, starting from the LSB for x >= 0

i. The number of bits that differ in n and n + 1 (when padded to the same length) = f(n) + 1 for n >= 0

ii. f(x) = largest value of a >= 0 such that (x % 2^a = 2^a - 1)

iii. To count the number of integers x in the range [0, n] (for n >= 0) such that f(x) = a, for some a >= 0, we do the following:

Let this count be y.
Now, y = number of integers x in the range [0, n] such that (y % 2^a = 2^a - 1 && y % 2^(a + 1) != 2^(a + 1) - 1)

The integers x in the range [0, n] such that x % b = b - 1 form the following AP:

b - 1, 2 * b - 1, ....,

where b = 2^a and this AP has g1 = (n - b) / b + 1 elements

The integers x in the range [0, n] such that x % d = d - 1 form the following AP:

d - 1, 2 * d - 1, ...,

where d = 2^(a + 1) and this AP has g2 = (n - d) / d + 1

Some rth term of the first AP and qth term of the second AP are equal iff r = 2 * q

Let g = (n - b) / b + 1
Then, it follows that the number of terms common in both the APs is exactly floor(g1 / 2). 

Thus y = g1 - floor(g1 / 2)  

48. n ^ (n + 1) ^ (n + 2) = n + 3 for any positive integer n

49. 

Let a[1, n] be an array of non-negative integers and let p[1, n] be its prefix XOR array.

Then,

1. XOR of subsegment a[l, r] = p[r] ^ p[l - 1] for any 1 <= l <= r <= n

2. a[r] = p[r] ^ p[r - 1] for any 1 <= r <= n

50. Consider the following loop

int cnt = 0;
for (int r = 1; r <= n; r++)
    for (int i = 1; i < r; i++)
        if (arr[i] >= arr[r])
            cnt++;

This is equivalent to counting the number of inversions (i < j and arr[i] >= arr[j]) in the array and can be done easily using the merge sort algorithm in O(nlog(n)) time.

int cnt = 0;
void merge(int l, int r, vector<int> &arr) {
    if (l == r) return;

    int sz = r - l + 1;
    merge(l, l + sz / 2 - 1, arr);
    merge(l + sz / 2, r, arr);

    vector<int> tmp(sz + 1);
    int i = l, j = l + sz / 2, k = 1;
    while (i <= l + sz / 2 - 1 && j <= r)
        if (arr[i] < arr[j]) tmp[k++] = arr[i++];
        else cnt += ((l + sz / 2) - i), tmp[k++] = arr[j++];

    while (i <= l + sz / 2 - 1)
        tmp[k++] = arr[i++];
    while (j <= r)
        tmp[k++] = arr[j++];

    for (int q = l; q <= r; q++)
        arr[q] = tmp[q - l + 1];
}

51. Greedy Dominance Principle

Greedy dominance is a principle where among multiple states representing the same "progress", some states are never better than others for any future transition.

Such states are called dominated and can be discarded safely.

Suppose a DP has states of the form dp[state]

A state A dominates state B if:
1. A and B represent the same stage (same length, same step)
2. For every possible future transition, A performs at least as well as B

Then, state B can be deleted without affecting the optimal answer.

i. Why dominance enables Greedy Algorithms

DP usually fails because there are too many states. Greedy dominance allows you to:

1. Keep only non-dominated states
2. Permanently discard inferior states
3. Reduce state count from quadratic/exponential to linear/logarithmic

This is how DP turns into Greedy + Binary Search

ii. LIS as an example:

Original DP:
dp[i] = LIS ending at i

Compressed State:
last[k] = smallest ending value of any LIS of length k

Dominance Rule:

Same length k -> smaller ending value dominates larger ending value

Hence, only one state per length is needed.

iii. The General Pattern

Greedy Dominance works when all the following holds:

1. States are comparable
There exists a total or partial order on states.

2. Future transitions depend on few parameters
The future only depends on current state parameters and not the entire history.

3. Monotonicity of transitions
If state A is better than state B now, then A remains better after any future step.

4. Exchange Argument Exists
You can prove: Any optimal solution using B can be transformed into one using A without reducing optimality.

iv. How to use this technique

1. Write the DP

dp[i][j] = best result after i steps with parameter j

2. Identify the "progress dimension"
Find what defines the stage:
i. length
ii. time
iii. number of items used
iv. number of operations

In LIS, it's length.

3. Find dominance within the same stage

Ask: Among all states within the same stage, which ones are always better.

4. Keep only minimal/maximal representative

Replace a set of states by a single best state per stage.

4. Maintain order and search

If the best states are ordered:
1. Use binary search
2. Use two pointers
3. Use monotonic queue/convex hull

General Template:

1. Define DP states.
2. Identify equivalent stages.
3. Define dominance relation.
4. Prove dominance via exchange argument.
5. Discard dominated states greedily.
6. Optimize transitions with data structure.

Greedy dominance works when you can totally order states so that “better now” implies “better forever”.

Binary Search LIS works exactly when DP states of equal length have a total dominance order by ending values.

We can use the binary search LIS method iff:

For increasing subsequences of the same length, the one with the smallest ending value dominates all others.

Formally, if two subsequences have equal length k and end at x <= y, then the one ending at x is always at least as extendable as the one ending at y. If this dominance holds, we can use the nlogn LIS. Otherwise, we need to resort to dp + segment/fenwick trees.

52. LIS

Longest Strictly Increasing Subsequence

int32_t main() {
    int n;
    cin >> n;

    vector<int> arr(n + 1), last(n + 2);
    for (int r = 1; r <= n; r++) cin >> arr[r];

    last[0] = LLONG_MIN;
    for (int r = 1; r <= n + 1; r++)
        last[r] = LLONG_MAX;

    // last[r] = smallest element that ends a strictly increasing
    // subsequence of length r for >= 1

    // Claim: last is always sorted in strictly increasing order
    // Proof: Suppose last[i] >= last[i + 1]
    // This means that there is a strictly increasing subsequence of length i + 1
    // that ends at last[i + 1]. The ith element of this subsequence (say x)
    // will be < last[i + 1]. Thus, there is a strictly increasing subsequence
    // of length i that ends at x < last[i], which is a contradiction.

    for (int r = 1; r <= n; r++) {
        int left = 0;
        int right = n + 1;

        // find the largest i such that last[i] < arr[r] for 0 <= i <= n
        while (right != left + 1) {
            int mid = left + (right - left) / 2;
            if (last[mid] < arr[r])
                left = mid;
            else 
                right = mid;
        }

        last[left + 1] = arr[r];
    }

    for (int r = n; r >= 1; r--)
        if (last[r] != LLONG_MAX) {
            cout << r << " ";
            break;
        }
    return 0;
}   

53. Number of Longest (Strictly)Increasing Subsequence

#include <bits/stdc++.h>
using namespace std;

#define int long long

int32_t main() {
    int n;
    cin >> n;

    vector<int> arr(n + 1);
    for (int r = 1; r <= n; r++) cin >> arr[r];

    // dp[r] is the length of the longest increasing subsequence
    // ending at r for 1 <= r <= n;

    // cnt[r] is the number of subsequences ending at r with length 
    // dp[r]

    int maxi = LLONG_MIN;
    int res = 0;
    vector<int> dp(n + 1), cnt(n + 1);
    for (int r = 1; r <= n; r++) {
        dp[r] = 1;
        cnt[r] = 1;

        for (int i = 1; i < r; i++)
            if (arr[i] < arr[r]) {
                if (dp[i] + 1 > dp[r]) dp[r] = dp[i] + 1, cnt[r] = cnt[i];
                else if (dp[i] + 1 == dp[r]) cnt[r] += cnt[i];
            }
        
        if (dp[r] > maxi) res = cnt[r], maxi = dp[r];
        else if (dp[r] == maxi) res += cnt[r];
    }

    cout << res << endl;
    return 0;
}

You track both the length and count of LIS ending at each position. When you find a longer subsequence, reset the count. When you find an equal-length one, add to the count.

54. 2D Longest Increasing Subsequence

1. Problem Statement

Given envelopes E = {(wr, hr)},

envelope i fits into envelope j iff wi < wj and hi < hj

Goal: find the maximum length envelope chain a1, a2, ... such that

a1 fits into a2, a2 fits into a3, ..., and so on

2. Naive DP

Define: dp[i] = max chain ending at envelope i

Transition
dp[i] = 1
for (int r = 1; r < i; r++)
    if (w[r] < w[i] && h[r] < h[i])
        dp[i] = max(dp[i], dp[r] + 1);

Time O(n^2)
Correct but slow.

3. Goal of Optimization

Reduce 2D partial order to a 1D LIS problem, which can be solved in O(n * log(n))

To do this, we must:

i. eliminate one dimension via sorting
ii. ensure the remaining dimension fully encodes validity

5. Sorting strategy

We sort envelopes by:
i. width increasing
ii. height decreasing if widhts are equal

6. Why sorting is required?

After sorting by width:

i < j -> wi <= wj

Thus,

i. width constraint is implicitly handled
ii. LIS only needs to check heights

But, this introduces a danger when wi = wj

7. Invariant required for LIS correctness

When we run LIS on heights, we assume:

if i < j and hi < hj, then envelope i can nest into j
This assumption is false when wi = wj

Thus, sorting must prevent height-increasing pairs with equal width.

8. What breaks if height is NOT sorted descending

Counterexample:

(5,4), (5,7)

widths equal → nesting impossible
but heights increase → LIS counts both

Result: invalid chain

9. Why height descending fixes it
For equal widths, sorting height descending gives:

(5, 7), (5, 4)

Heights are decreasing, so:
i. LIS can pick at most one
ii. Equal width envelopes never form a chain

10. Key invariant after sorting

After sorting by (width increasing, height decreasing):

If i < j and hi < hj, then necessarily wi < wj
This guarantees that every increasing height subsequence corresponds to a valid nesting chain.

11. Reduction to LIS

After sorting, define sequence:

H = {h1, h2..., hn}

Now solve: LIS of H
This gives the maximum nesting depth.

In general, when we want to reduce the dimensions of a LIS to a single dimensional LIS on some dimension x, we need to ensure the following invariant:

that every increasing x subsequence corresponds to a valid chain

55. Suppose we have an array a[1, n] for some n >= 1 and we want to find the minimum number of deletions required to sort the array in strictly increasing order.

To do this, find the length longest strictly increasing subsequence, say its x.

Then, the answer is n - x.

56. If you store a tree in an adjacency list format and do a DFS on it using the following signature

int dfs(int node, int parent, vector<vector<int>> &tree);

and the dfs is called from some root x as:

dfs(x, -1, tree)

then, the correct way to check whether the current node is a leaf node is:

(tree[node].size() == 1 && parent != -1)

A node is a leaf node iff the size of its adjacency list is 1 AND it's NOT the root node.
 
57. 
Let n >= 1
Let S = {1, 2, ..., n}

1. For any 1 <= r <= (n * (n + 1)) / 2, there exists a subset s of S such that sum(s) = r.

To find this set s for the sum r, we do the following greedy construction:

vector<int> s;
int sum = r;

for (int i = n; i >= 1; i--)
    if (sum >= i) sum -= i, s.push_back(i);

2. Let 1 <= k <= n
Let s1 = k(k + 1) / 2 (sum of the smallest k integers in S).
Let s2 = k * (2n - k + 1) / 2 (sum of the largest k integers in S)

Then, for any s1 <= r <= s2, there is a subset s of S of size k such that sum(s) = r.

To find this set s for sum r, we use the following greedy construction:

i. Start from the minimum set

A = {1, 2, ..., k}
d = r - (k * (k + 1)) / 2

ii. Greedy right-to-left shifting

Write A = {a1 < a2 < ... < ak}, a[i] = i for 1 <= i <= k

For i = k, k-1, ... ,1:
    di = min(d, n - k + i - a[i])
    set ai = a[i] + di, d = d - di
    stop when d = 0

iii. We will finally have a subset s of S of size k that sums to r.

3. Suppose you want to find a subset s of S of size k such that summation(e; e belongs to s) - k = j, i.e,

summation(e - 1; e belongs to s) = j

Thus, the problem becomes the following:

Find a subset s' of S' = {0, 1, ..., n - 1} such that sum(s) = j

The final subset is the following s = {e + 1 | e belongs to s'}

58. Let a <= b 
Let f(x) = |x - a| + |x - b| for all x

Then, 

1. If x < a, f(x) = a + b - 2 * x
2. If a <= x <= b, f(x) = b - a
3. If (x > b), f(x) = 2 * x - a - b

Thus, the minimum value of f occurs in the entire range [a, b].

59. 

Let n >= 1
Let A[1, n] be a non-decreasing array of integers.

Let f(x) = summation(|x - a[r]| for 1 <= r <= n) for all integers x.

1. If n is even, the median interval of A is [A[n / 2], A[n / 2 + 1]].
2. If n is odd, the median interval of A is [A[n / 2 + 1], A[n / 2 + 1]]. (the interval contains a single value)
3. The minimum value of f is achieved in the
median interval of A.

60. Let n, m >= 1.
Consider a grid having n rows and m columns, where some cells may be blocked.
Consider some point (y, x) on the grid where 1 <= y <= n, 1 <= x <= m that is not blocked. Consider a walk on this grid from this point with each step being one of up, down, left or right.

1. If we use R right and L left steps to reach some non-blocked reachable cell (r, c) using some path, then, R - L = c - x.
2. If we use U up and D down steps to reach some non-blocked reachable cell (r, c) using some path, then, U - D = y - r.

61. 0–1 BFS

Problem setting:
We are given a graph where each edge weight is either 0 or 1.
We want to find the minimum cost (shortest path) from a source node to all other nodes.

Using Dijkstra works, but it is slower than necessary.
0–1 BFS is an optimized shortest-path algorithm for this exact situation.


When to use 0–1 BFS:
- Edge weights are only 0 or 1
- We need shortest paths / minimum cost
- We want O(V + E) time instead of O(E log V)


Core idea:
0–1 BFS is a special case of Dijkstra that replaces the priority queue
with a deque.

Observation:
- Weight 0 edge → distance does not increase
- Weight 1 edge → distance increases by exactly 1

Therefore:
- Nodes reached via weight 0 edges should be processed immediately
- Nodes reached via weight 1 edges should be processed later

We achieve this by:
- push_front() for weight 0 edges
- push_back() for weight 1 edges

Algorithm:
1. Initialize all distances to infinity
2. Set distance of source = 0
3. Push source into deque
4. While deque is not empty:
   - Take node from the front
   - Relax all outgoing edges
   - Push neighbors based on edge weight

Pseudocode:

int src = 1;
deque<int> dq;
vector<long long> dist(n + 1, INF);

dist[src] = 0;
dq.push_back(src);

while (!dq.empty()) {
    int u = dq.front();
    dq.pop_front();

    for (auto [v, w] : graph[u]) {
        // w is either 0 or 1
        if (dist[v] > dist[u] + w) {
            dist[v] = dist[u] + w;

            if (w == 0)
                dq.push_front(v);
            else
                dq.push_back(v);
        }
    }
}

Why this works:
The deque always keeps nodes in non-decreasing order of distance.
- Weight 0 edges keep the same distance → front
- Weight 1 edges increase distance → back

This guarantees correctness like Dijkstra, but faster.

Time and Space Complexity:
- Time: O(V + E)
- Space: O(V + E)

62. Let G be a connected graph having n vertices, where n >= 1.

Let x be the number of nodes at an even distance from node 1. Let y be the number of nodes at an odd distance from node 1.

Then, x <= floor(n / 2) or y <= floor(n / 2).

63. Preorder v/s Postorder DFS

// Preorder -> add first and then visit neighbours
// Postorder -> add only when all the neighbors are added

vector<int> pre, post
void dfs(int node) {
    pre.push_back(node);
    for (int v : graph[node])
        if (!vis[v])
            vis[v] = true, dfs(v);
    post.push_back(v);
}

64. Let n, m >= 1.

Say we traverse a 4-connected grid (Up, Down, Left, Right). Each cell is a vertex and each shared boundary is an edge.

Each internal cell has 4 neighbours, but each edge is shared between 2 cells. 

Horizontal edges: each row has m - 1 of them across n rows, giving n * (m - 1). 

Vertical Edges: each column has n - 1 across m columns, giving m * (n - 1).

Thus, the total number of edges in this implicit graph is 2 * n * m - n - m, i.e, O(n * m).

65. Cycle Detection in Directed Graphs (DFS with 3 States)

In a directed graph, a cycle exists if during DFS we encounter a back edge.
A back edge is an edge that points to a node already present in the current
DFS recursion path.

A simple visited[] boolean array is NOT sufficient because it does not tell
whether a node is currently in the recursion stack or already fully processed.
Hence, we use three states.

--------------------------------------------------
i. Node States
--------------------------------------------------
For every node u, maintain state[u]:

State 0 (Unvisited):
- The node has not been visited yet.

State 1 (Visiting):
- The node is currently being processed.
- It is present in the current DFS recursion stack.

State 2 (Visited):
- The node and all its descendants are completely processed.
- It is NOT in the recursion stack.

--------------------------------------------------
ii. Key Observation
--------------------------------------------------
If during DFS from node u, we encounter an edge u -> v such that:
    state[v] == 1
then v is already in the current DFS path.
This edge is a back edge, which implies a cycle exists.

--------------------------------------------------
iii. DFS Algorithm
--------------------------------------------------
1. Mark the current node as Visiting (state = 1).
2. For every outgoing edge to a neighbor:
   - If the neighbor is Unvisited, recursively DFS on it.
   - If the neighbor is Visiting, a cycle is detected.
3. After all neighbors are processed, mark the node as Visited (state = 2).

--------------------------------------------------
iv. Code
--------------------------------------------------

int state[N]; // 0 = unvisited, 1 = visiting, 2 = visited

// returns true if a cycle is found in the graph reachable from node u
bool dfs(int u) {
    state[u] = 1; // mark as visiting

    for (int v : graph[u]) {
        if (state[v] == 0) {
            if (dfs(v))
                return true;
        }
        else if (state[v] == 1) {
            return true; // back edge found -> cycle
        }
    }

    state[u] = 2; // mark as fully visited
    return false;
}

--------------------------------------------------
v. Handling Disconnected Graphs
--------------------------------------------------
Since the graph may be disconnected, DFS must be started from every unvisited
node.

bool hasCycle(int n) {
    for (int i = 1; i <= n; i++) {
        if (state[i] == 0) {
            if (dfs(i))
                return true;
        }
    }
    return false;
}

--------------------------------------------------
vi. Why This Works
--------------------------------------------------
- Nodes with state == 1 represent the current DFS path.
- Reaching such a node again means we looped back before finishing it.
- This loop corresponds to a cycle in a directed graph.

--------------------------------------------------
vii. Complexity
--------------------------------------------------
Time Complexity:  O(V + E)
Space Complexity: O(V)  (recursion stack + state array)

--------------------------------------------------
viii. One-Line Summary
--------------------------------------------------
A directed graph contains a cycle if DFS encounters a node that is already in
the Visiting (1) state.

65. Bipartite Graphs

An undirected graph G = (V, E) is bipartite if there is a 2-partition A, B of the set V such that

1. A and B are non-empty.
2. For any {u, v} belonging to E, either (u is in A and v is in B) or (u is in B and v is in A).

If G is bipartite, this 2-partition A, B is not necessarily unique. However, if G is bipartite and G is connected and you fix the partition to which some specific node belongs to, then the partition which all other nodes belong to also becomes fixed. The structure forces every other node into a specific side.

A graph is bipartite iff it has no cycles of odd-length.

A disconnected graph is bipartite iff its every connected component is bipartite.

Thus, a bipartite graph is one where you can split all nodes into two groups such that every edge connects a node fromn one group to a node in the other. No edge connects two nodes within the same group.

A graph is 2-colorable if we can color all the nodes of the graph in one of two colors (say A and B) such that no edge connects two nodes of the same color.

A graph is bipartite iff its 2-colorable. Similar to the case of bipartiteness, if a graph is 2-colorable, its coloring is not unique. However, if we fix the color of some specific node, the color of the rest of nodes also become fixed. The structure forces every other node into a specific color.

Bipartite graphs model two-sided relationships.

// Returns true if the component that node x belongs to is bipartite; false otherwise
bool isBipartite(int x) {
    queue<int> que;
    vector<int> color(n + 1, -1);

    color[x] = 0, que.push(x);
    while (!que.empty()) {
        int node = que.front(); que.pop();

        for (int v : graph[node])
            if (color[v] == -1)
                color[v] = !color[node], que.push(v);
            else if (color[v] == color[node])
                return false;
    }
    return true;
}

66. Trees

Let G = (V, E) be a finite undirected graph with |V| = n vertices and |E| = m edges.

The following statements are then equivalent

1. G is a tree.
2. G is connected and acyclic.
3. G is acyclic and m = n - 1.
4. G is connected and m = n - 1.
5. For any u, v in V such that u != v, there is a unique simple path between u and v in G.
6. G is connected and removal of any edge disconnects G.
7. G is acyclic and adding a new edge between any two distinct vertices in G creates exactly one cycle in G.

Let G = (V, E) be a tree.

Fix an arbitrary vertex r ∈ V.
We now define the rooted tree structure with root r.

------------------------------------------------------------

i. Root

Definition:
The chosen vertex r ∈ V is called the root of G.

Justification:
Since G is connected, for every vertex v ∈ V,
there exists a path from r to v.
Hence rooting at r is always valid.

------------------------------------------------------------

ii. Parent

Let v ∈ V such that v ≠ r.

Definition:
The parent of v, denoted parent(v), is the unique vertex
immediately preceding v on the unique simple path from r to v.

Justification (Well-definedness):
• In a tree, between r and v there exists exactly one simple path.
• On that path, v has exactly one predecessor.
• Hence parent(v) exists and is unique.

Conclusion:
Every non-root vertex has exactly one parent.
The root r has no parent.

------------------------------------------------------------

iii. Child

Let u, v ∈ V with u ≠ v.

Definition:
v is a child of u if parent(v) = u.

Observation:
Each vertex may have zero or more children.
Edges can now be viewed as directed from parent → child,
though the underlying graph remains undirected.

------------------------------------------------------------

iv. Depth

Let v ∈ V.

Definition:
depth(v) = length (number of edges) of the unique simple path from r to v.

Justification:
Since the path from r to v is unique, its length is uniquely determined.
Hence depth is well-defined.

Special case:
depth(r) = 0.

------------------------------------------------------------

v. Ancestor

Let u, v ∈ V with u ≠ v.

Definition:
u is an ancestor of v if u lies on the unique simple path from r to v.

Equivalently:
u is an ancestor of v if repeated application of parent(·)
starting from v eventually reaches u.

Properties:
• The root is an ancestor of every vertex.
• Every vertex is its own ancestor if reflexive definition is allowed
  (optional convention — state clearly in notes).

------------------------------------------------------------

vi. Descendant

Let u, v ∈ V with u ≠ v.

Definition:
v is a descendant of u if u lies on the unique simple path from r to v.

Equivalently:
v is in the subtree rooted at u.

This is simply the reverse relation of ancestry.

------------------------------------------------------------

vii. Subtree

Let v ∈ V.

Definition:
Let

A = { u ∈ V | v lies on the unique simple path from r to u }.

That is, A consists of v and all its descendants.

Let

B = { {e, f} ∈ E | e ∈ A and f ∈ A }.

Define G' = (A, B).

Claim:
G' is a tree.

Justification:
1) Connected:
   For any two vertices in A, the unique path between them in G
   lies entirely inside A (because descendants share ancestry under v).

2) Acyclic:
   G has no cycles.
   Any subgraph of an acyclic graph is also acyclic.

Hence G' is connected and acyclic,
so G' is a tree.

------------------------------------------------------------

Key Structural Consequences

1) Every non-root vertex has exactly one parent.
2) Total number of parent-child edges = |V| − 1.
3) The hierarchy (parent/child structure) arises purely from:
      Tree
      ⇒ Unique simple paths
      ⇒ Unique predecessor toward root
      ⇒ Well-defined parent
      ⇒ Hierarchical structure.

Important Insight:
Hierarchy is NOT part of the definition of a tree.
It is a consequence of the unique path property.

67. 

i. Tree DP

1. Define what you want to compute for each node.
2. Express it in terms of the answers for the children.
3. Write a recursive function that combines child results.

Tree DP is bottom-up: you solve for leaves first, then combine results as you return up the tree. 

ii. The diameter of a tree is the length of the longest simple path between any two nodes of the tree.

68. Lowest Common Ancestor (LCA):
For two vertices u, v ∈ V, their Lowest Common Ancestor,
denoted LCA(u, v), is the unique vertex w such that:

1) w is an ancestor of both u and v, and
2) depth(w) is maximal among all common ancestors of u and v.

Equivalently:
w is the common ancestor of u and v that is farthest from the root.

Uniqueness:
The LCA exists and is unique because:
• The set of ancestors of any vertex is exactly the vertices
  on the unique path from the root to that vertex.
• The intersection of the ancestor sets of u and v
  is a non-empty finite chain (it contains at least r).
• In this chain, there is a unique deepest element.

69. Path structure between two nodes in a Tree


Let u, v ∈ V be distinct vertices.
Let w = LCA(u, v).
Let p be the unique simple path between u and v.

Claim 1: w lies on p.

Proof:
There is a unique simple path from u to w (call it p1),
since the tree has a unique path between any two vertices.

There is also a unique simple path from w to v (call it p2).

Concatenating these gives a path:

    p1 + p2

which is a path from u to v.

Since G is a tree, there exists exactly one simple path
between u and v. Therefore:

    p = p1 + p2.

Hence w lies on p.

---------------------------------------

Claim 2: The path p does not pass through the parent of w.

Proof:
Assume for contradiction that p passes through parent(w).

Then parent(w) would lie on the path from u to v.
Hence parent(w) would be an ancestor of both u and v.

But parent(w) has strictly smaller depth than w.
This contradicts the definition of w as the
lowest (deepest) common ancestor.

Therefore, p cannot go above w.
Hence w is the highest (closest to root) vertex on p.

-------------------------------------

For any two vertices u and v in a rooted tree:

The unique simple path between (say p) them consists of:

    (path from u to LCA(u, v))
    +
    (path from LCA(u, v) to v)

The vertex LCA(u, v) is the unique “turning point” of the path, the only vertex where the direction changes from moving upward toward the root to moving downward away from the root.

70. Decomposition of the Unique Simple Path in a Rooted Tree

Let G = (V, E) be a tree rooted at r.
Let u, v ∈ V be distinct vertices.
Without loss of generality, assume depth(v) ≤ depth(u).

Let p be the unique simple path between u and v.
Let w = LCA(u, v).

We distinguish two cases.

------------------------------------------------------------
Case 1: LCA(u, v) = v
------------------------------------------------------------

In this case, v is an ancestor of u.

Therefore, the unique path from u to v is exactly the
unique path from u upward to v.

Hence:

    p = (unique path from u to v).

Equivalently, p lies entirely inside the subtree rooted at v.

More precisely:
Let c be the child of v that lies on the path from v to u.
Then the path p can be written as:

    p = v + (path entirely contained in the subtree rooted at c).

Thus, when one vertex is an ancestor of the other,
the entire path is contained in a single subtree.

------------------------------------------------------------
Case 2: LCA(u, v) = w and w ≠ v
------------------------------------------------------------

In this case, neither u nor v is an ancestor of the other.

Let cu be the child of w that lies on the path from w to u.
Let cv be the child of w that lies on the path from w to v.

Since w is the lowest common ancestor,
cu ≠ cv.

Then the unique path p between u and v can be decomposed as:

    p = (path from u up to w)
        +
        (path from w down to v).

Equivalently:

    p = p1 + w + p2

where:
• p1 is a path contained entirely in the subtree rooted at cu,
• p2 is a path contained entirely in the subtree rooted at cv.

Thus, the path moves upward from u to w,
and then downward from w to v,
switching subtrees at w.

------------------------------------------------------------
Structural Summary
------------------------------------------------------------

For any two vertices u and v in a rooted tree:

• If one is an ancestor of the other,
  the unique path lies entirely inside one subtree.

• Otherwise,
  the path ascends from u to their LCA,
  then descends into a different child subtree of the LCA.

In particular:

Every simple path in a rooted tree
is either contained in a single subtree,
or has a unique turning point at LCA(u, v),
where it moves from one child subtree of that node
to another.

71. Triangle Inequality

Let T is a tree and let x, y, z be three vertices in T. Then,

dist(x, z) <= dist(x, y) + dist(y, x)

This is the Tree Triangle Inequality.

72. Tree Diameter

1) Definition

Let T = (V, E) be a tree.

• The diameter of a tree is the length (number of edges) of the
  longest simple path between any two vertices.

• If the longest path is between nodes a and b:
    - (a, b) is called a diameter pair
    - a and b are called diameter endpoints


----------------------------------------
2) Diameter Endpoints are Leaves
----------------------------------------

Claim:
If (a, b) is a diameter pair, then both a and b are leaves.

Reason:
If a were not a leaf, it would have a neighbor outside the
diameter path. Extending the path through that neighbor would
produce a longer path — contradicting maximality.

Therefore, diameter endpoints must be leaves.


----------------------------------------
3) Key Fact (Used in Double BFS)
----------------------------------------

Let u be any node in the tree.
Let s be a node farthest from u.

Then:
    s is a diameter endpoint.

(Intuition: from any starting point in a tree, the farthest
vertex must lie at one extreme of the tree.)

This property is unique to trees. In general graphs with cycles, the farthest node from a random start is not guaranteed to be a diameter endpoint. Trees have exactly one path between any pair of nodes, whihc makes this argument work.


----------------------------------------
4) Double BFS Algorithm
----------------------------------------

Goal: Find the diameter of a tree.

Step 1:
    Pick any node u.
    Run BFS (or DFS) from u.
    Let s = farthest node from u.

    (By the key fact, s is a diameter endpoint.)

Step 2:
    Run BFS from s.
    Let t = farthest node from s.

Then:
    distance(s, t) = diameter of the tree.


----------------------------------------
5) Why It Works (Intuition)
----------------------------------------

• First BFS moves to one extreme of the tree.
• Second BFS stretches from that extreme to the opposite extreme.
• That second distance is the longest possible path.


----------------------------------------
6) Complexity
----------------------------------------

Each BFS runs in O(n).

Total time complexity:
    O(n)

Space complexity:
    O(n)


----------------------------------------
Final Summary
----------------------------------------

• Diameter = longest simple path in a tree.
• Diameter endpoints are leaves.
• Farthest node from any node is a diameter endpoint.
• Double BFS finds the diameter in linear time.

============================================================
TREE CENTRE — FULLY RIGOROUS AND COMPLETE PROOF
(With Correct Proof that All Diameters Have Same Middle)
============================================================

Let T be a tree.

------------------------------------------------------------
I. DEFINITIONS
------------------------------------------------------------

1) Distance

For vertices u, v:

    dist(u, v) = number of edges on the unique u–v path.

(Trees have exactly one simple path between any two vertices.)

------------------------------------------------------------

2) Eccentricity

For vertex u:

    e(u) = max_v dist(u, v)

------------------------------------------------------------

3) Diameter

    D = max_{u,v} dist(u, v)

A pair (A, B) with dist(A, B) = D
is called a pair of diameter endpoints.

------------------------------------------------------------

4) Centre

A vertex c is a centre if

    e(c) = min_x e(x)

A tree has either:
    • one centre, or
    • two adjacent centres.

We now prove everything rigorously.

============================================================
PART 1 — STRUCTURAL LEMMA
============================================================

Lemma 1.
Let A, B be diameter endpoints.
Then for every vertex x:

    e(x) = max(dist(x, A), dist(x, B))

Proof:

Let x is a vertex of T.
The farthest node from x is a diameter end-point. Thus,

e(x) = max(dist(x, A), dist(x, B)).

============================================================
PART 2 — CENTRE LIES ON EVERY DIAMETER
============================================================

Lemma 2.
Every centre lies on the path A–B.

------------------------------------------------------------
Proof
------------------------------------------------------------

Let c be a centre.

Assume c is NOT on A–B.

Let p be the unique vertex of A–B
where the branch containing c attaches.
(Existence and uniqueness follow from acyclicity.)

Because paths are unique:

    dist(c, A)
    = dist(c, p) + dist(p, A)

    dist(c, B)
    = dist(c, p) + dist(p, B)

By Lemma 1:

    e(c)
    = max(dist(c, A), dist(c, B))
    = dist(c, p)
      + max(dist(p, A), dist(p, B))
    = dist(c, p) + e(p)

Since c ≠ p:

    dist(c, p) > 0

Therefore:

    e(c) > e(p)

This contradicts minimality of e(c).

Hence c lies on A–B.


============================================================
PART 3 — MINIMIZING ECCENTRICITY ALONG A DIAMETER
============================================================

Now restrict to vertices on A–B.

Let D = dist(A, B).

Let x be a vertex on A–B
with:

    t = dist(A, x)

Then:

    dist(x, A) = t
    dist(x, B) = D − t

By Lemma 1:

    e(x) = max(t, D − t)

Define:

    f(t) = max(t, D − t)

We minimize f(t).

Case 1: D even

Minimum occurs uniquely at:

    t = D/2

Thus there is exactly ONE centre.

Case 2: D odd

Minimum occurs exactly at:

    t = floor(D/2)
    t = ceil(D/2)

Thus exactly TWO adjacent centres.

Therefore:

The centre(s) are exactly
the middle vertex (or two middle vertices)
of A–B.


============================================================
PART 4 — UNIQUENESS OF THE MIDDLE OF ALL DIAMETERS
============================================================

Now we prove the crucial statement:

THEOREM.
All diameter paths of a tree
have the same middle vertex
(if D even)
or the same middle edge
(if D odd).

------------------------------------------------------------
Proof
------------------------------------------------------------

Let A–B be one diameter.
Let C be its middle set
(one vertex or two adjacent vertices).

From Part 3:

C is exactly the set of vertices
with minimum eccentricity.

Thus:

    C = { x : e(x) = min_y e(y) }

Now let U–V be ANY other diameter.

Since dist(U, V) = D,
consider its midpoint(s).
Call this set C'.

We prove:

    C' = C


Step 1:
The midpoint(s) of U–V minimize
the function:

    x ↦ max(dist(x, U), dist(x, V))

Along U–V,
this is minimized exactly at its middle(s).


Step 2:
But since U–V is also a diameter,
we may apply Lemma 1 to U and V:

    e(x) = max(dist(x, U), dist(x, V))

Thus the midpoint(s) of U–V
are exactly the vertices
of minimum eccentricity.


Step 3:
But the set of vertices
with minimum eccentricity
was already shown to be C,
which is uniquely determined
from A–B.

Hence:

    C' = C

Therefore every diameter
has the same midpoint(s).

============================================================
FINAL STRUCTURAL RESULT
============================================================

• The centre of a tree is exactly
  the vertex (or two adjacent vertices)
  of minimum eccentricity.

• These vertices are exactly
  the middle vertex/edge
  of any diameter path.

• Therefore all diameters
  share the same middle point(s).

• A tree has either:
      – one centre (D even)
      – two adjacent centres (D odd)

============================================================


73. Tree Centres

A Centre of a tree is a node that minimizes the distance to the furthest node. 

If you root the tree at a centre, the height of the tree is minimized. 
The middle vertex (if diameter is even)/edge (if diameter is odd) of all the diameter paths, of the tree are the same.

If the diameter of a tree is even, there is exactly one centre, the middle node on a diameter path.

If the diameter of a tree is odd, there are two centres, the two middle nodes on a diameter path.

74. Topological Peeling Algorithm for finding Tree Centres

Given: A tree T with n vertices.
Goal: Find its centre(s).

i. Idea:

Repeatedly remove all leaves (degree = 1 vertices) layer by layer.
Eventually:

a. Either one vertex remains -> unique centre
b. Or two adjacent vertices remain -> two centres

It works because a tree shrinks inwards symmetrically from its outermost layer.

ii. Algorithm

Input: Tree T with n vertices

1. If n = 1: return that vertex.
2. Compute degree of every vertex.
3. Initialize a queue Q with all leaves (vertices whose degree = 1).

4. remaining_vertices = n

5. while (remaining_vertices > 2):
    new queue Q'
    let k = size of Q
    remanining_vertices -= k

    for each leaf u in Q:
        remove u from the tree
        for its neighbours v:
            decrease degree[v] by 1
            if degree[v] becomes 1:
                push v into Q'

    replace Q with Q'

6. Return the remaining 1 or 2 vertices in the queue.

Time Complexity: O(n)
Each edge and vertex is processed once.

75. Modifying a Tree in-place

To modify a tree in place, you do not need to create a new tree. You need to identify the local change (swap, delete, rotate) and trust recursion to apply it everywhere.

Always return the root after modification. Even if you change the tree in place, returning the root makes the function composable and easier to test.