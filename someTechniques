1. In questions that have the count of somethings changing when an operation is done and the total number of somethings remain constant, check how the parity of the count of things changes after an operation

2. In questions in which we can perform any operation from a group of operations, one after other, there is usually an optimal order for the
application of these operations (i.e., something like all operation 1s must occur before any operation 2s and so on) (see random/gettingZero)

3. For any x > 1, __builtin_ctzll(x) is the power of 2 in the prime factorization of x

4. If n is odd and >= 3, then
n, n - 1 and n - 2 have no prime
factors in common and
LCM(n, n - 1, n - 2) = n * (n - 1) * (n - 2)

5. For n >= 2, gcd(n, n - 1) = 1

6. Let x >= 1
Then, there can be atmost 1 number in the sequence

x, x + 1, x + 2, ..., x + (r - 1)

for some r >= 1 that is divisible by a number >= r

7. Count/Find triplets 1 <= i < j < k <= n that satisfy some conditions in O(n^2)

for (int k = 2; k <= n - 1; k++) {
	// iterate through [1, k - 1] to choose a suitable i

	// iterate through [k + 1, n] to choose a suitable j

	// check if (i, j, k) is a suitable pair
}

Thus, if we have to find a triplet, fix the middle one and solve two independent problems (if possible)

8. Let n >= 2
Let a1, a2, ..., an is a sequence of integers

If a1 <= an, then there are
two consecutive elements x, y
of the sequence such that x <= y

9. Let p(x1, y1) and q(x2, y2) be two points on a line y = mx + c.

Then, distance(p, q) = |x2 - x1| * root(1 + m * m)

For any integral m != 0, distance(p, q) is always irrational

10. For two distinct points P and Q in a plane to not have integral distance between them, it's necessary for them to be have different rows or different columns

11. For constructive questions, say we have to construct the maximal set satisfying some conditions, we can adopt the following approach:

i. Get an upper bound on the size of the set
ii. Do something to construct a set of that size

12. For x >= 0

i. floor(x / 2) = x >> 1
ii. ceil(x / 2) = (x >> 1) + (x & 1)

13.

Consider a sorted array v[1, n]

Let f(r) = |r - v[1]| + |r + 1 - v[2]| + ... |r + n - 1 - v[n]|

         = |v[1] - (r + 0)| + ... + |v[n] - (r + (n - 1))|

We can calculate this in O(log(n)) with O(n) precomputation as follows:

1. Construct an array s[1, n] such that

s[i] = v[i] - (i - 1) for 1 <= i <= n

3. Construct the prefix array of s, say p[0, n]

p[0] = 0 and p[r] = p[r - 1] + s[r] for 1 <= r <= n

2. Find the first index k in the array v[1, n] such that v[k] >= r

If such a k does not exist, then
f(r) = r * n - p[n]

If k == 1, then f(r) = p[n] - r * n

Otherwise,

f(r) = (p[n] - p[k - 1]) - r * (n - k + 1) + r * (k - 1) - (p[k - 1])

14.

Let n >= 1
Let S = {1, 2, ..., n}
Let A = {r is a natural number such that 1 <= r <= n * (n + 1) / 2}

Then, for any a in A, there is a subset s of S such that sum(s) = a
Also, there is no subset s of S such that
sum(s) > n * (n + 1) / 2

15.

Let n >= 3
Let l1, l2, ... , ln be positive integers

A polygon with n sides can be formed with
these integers as its side lengths iff

2 * max(l1, l2, ..., ln) <= l1 + l2 + ... + ln

, i.e, no side-length is greater than all the other side-lengths combined

16. In a problem where we are to count
the number of pairs of indexes (i, j) where 1 <= i < j <= n in an array arr[1, n] satisfying some condition, we can permute (usually, the permutation of the array used is the sorted one) the array beforehand and solve the problem if

i. Condition depends on values, not positions
ii. The number of pairs is invariant under the permutation
iii. Counting pairs/combinations/subsets
iv. Answer is unchanged by reordering (permutation invariant)

We cannot permute the array if

i. Problem involves subarrays/prefixes/suffixes
ii. Ordering of the elements matters
iii. Distance/position constraints are essential

16.

Let k >= 0

Let A[1, n] and B[1, m] be two integer arrays such that

0 <= A[r] < 2^k for 1 <= r <= n

Let f[s] = Summation from r = 1 to n of (A[r] ^ B[s]) for 1 <= s <= m

Suppose we want to find max(f[s] for 1 <= s <= m) in O(n + m)

We can do this as follows:

vector<int> bit(k);
for (int r = 1; r <= n; r++)
	for (int b = 0; b <= k - 1; b++)
		if (A[r] & (1 << b)) bit[b]++;

// We can now calculate each f[s] for 1 <= s <= m in O(1) now after this precomputation that is done in O(n)

int maxi = LLONG_MIN;
for (int r = 1; r <= m; r++) {
	int sum = 0;
	for (int b = 0; b <= k - 1; b++)
		if (B[r] & (1 << b)) sum += bit[b] * (1 << b);
		else sum += (n - bit[b]) * (1 << b)
}

17.

Let x >= 1
Let z is the smallest power of two >= x

Then x ^ y, for y >= 1 and y != x, divides y implies that y <= z - 1

18.

Let A[1, n] and B[1, k] are sorted
array of integers

Let f: A -> B

Suppose we are to find an assignment f such that

1. summation |A[r] - B[f[r]]| for 1 <= r <= n is minimized

or

2. max(|A[r] - B[f[r]]| for 1 <= r <= n) is minimized

Then, it is not optimal to assign in a crossed manner, i.e,

If we assign B[j] to some A[r], then, it is not optimal to assign A[r + 1] to any B[s] for s < j

19. Chicken McNugget Theorem

If you have two positive integers a and b that are coprime (that is, gcd(a, b) = 1), then, the largest
integer that cannot be written in the form ax + by, where x and y are non-negative integers, is ab - a - b
After this number, every larger number is representable

20.

Let f(x) = 11...1 (total of x ones)
Then, for any x >= 2, f(x) = 11 * a + 111 * b for some non-negative integers a and b

Rough Proof:

There are three possible cases for x:

1. x = 3 * r for some r >= 1
Then f(x) = 111 * pw(10, x - 3) + 111 * pw(10, x - 6) ... + 111 * pw(10, 0) = 111 * y for some y >= 0

2. x = 3 * r + 1 = 3 * (r - 1) + 4 for r >= 2
Then f(x) = 111 * pw(10, x - 3) + .. + 111 * pw(10, 4) + 11 * 100 + 11 = 111 * a + 11 * b for some a, b >= 0

3. x = 3 * r + 2
Then f(x) = 111 * pw(10, x - 3) + ... + 111 * pw(10, 2) + 11 * 100 = 111 * a + 11 * b for some a, b >= 0

21.

Let n >= 2
Let arr[1, n] be an array of non-negative integers

Suppose we have to partition arr into consecutive subarrays such that the XOR of all the subarrays are equal.
A partition with 1 subarray is always possible (the trivial partition {{1, 2, ..., n}})

Suppose we need to have the number of partitions >= 2
Let p[0] = 0 and p[r] = p[r - 1] ^ arr[r] for 1 <= r <= n

If p[n] = 0, then we can always partition arr[1, n] into two subarrays with equal XORs ->
{{1, ..., r}, {r + 1, ..., n}} where the XORs of both the subarrays are equal
Such two subarrays always exist since if the XOR of some numbers is 0, then its possible iff the numbers are equal
And since p[n] = arr[1] ^ ... ^ arr[n] = 0, then we will always have p[n] = x ^ x where the first x is the xor of some prefix, and the
second x is xor of the rest of the array

If p[n] > 0, then, no even length partition exists since XOR of an equal number of even things is zero
So if the array can be partitioned, it will have an odd length partition
Now, since the XORs of the partitioned subarrays are equal and the partitioned subarrays are odd in number,
the XORs of these partitioned subarrays = p[n]
So, we can check if such a partition exists by the following code:

int cnt = 0, l = 1, r = 1;
while (l <= r) {
    l = r;
    while (r <= n && (p[r] ^ p[l - 1] != p[n]) r++;
    if (r <= n && p[r] ^ p[l - 1] == p[n]) cnt++;
    l = r + 1;
}

if (cnt >= 1) return true;
else return false;

22.

When during thinking of a transition for dp[r], you find that the transition involves dependency
on values of some dp[r + l] where l > 0, then, think of push-dp or recursion to solve the problem.

23. String DP Patterns

Four things to remember:

The "prefix to prefix" pattern (comparing beginnings of both strings) is the foundation of all string DP.

1. String DP compares prefixes. dp[i][j] relates prefix of string A to prefix of string B
2. Match v/s operate. When characters match, you get a free transition.
3. Three choices become one min. Replace, delete, insert map to three neighbors.
4. Base cases are edge strings. Empty-to-something costs the length.

For example consider the problem Edit Distance.
Let A[1, n] and B[1, m] be the two strings

We define dp[i][j] is the minimum number of operations required to convert A[1, i] into B[1, j]
Here are the transitions

dp[0][0] = 0
dp[0][j] = j for all 1 <= j <= m
dp[i][0] = i for all 1 <= i <= n

dp[i][j] = dp[i - 1][j - 1] if A[i] = B[j]
         = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + 1) otherwise

The final answer is dp[n][m]

24. Resource Tracking DP
dp[i][j] is the answer after i steps using j of some resource

25. Grid DP
dp[i][j] is the answer for cell at row i, column j

26. String DP
dp[i][j] is the answer comparing first i characters of one string to first j of another

27. Ordered Inclusion-Exclusion using DP (First-violation DP pattern)

This technique applies to problems of the following type:
1. We want to count objects (paths, sequences, walks, strings, processes, etc.)
2. Objects are built step by step (or move monotonically)
3. There are forbidden states / events
4. The total state space is too large for naive DP
5. Direct inclusion-exclusion over forbidden sets is exponential and impossible

The technique relies on one crucial property: Every invalid object has a unique earliest violation
This is the entire foundation. It matters because if each bad object can be charged to exactly one first
bad event, then we can subtract bad objects once and only once. This avoid exponential inclusion-exclusion.

Let:
Object = things we want to count
States = ordered checkpoints / events / positions
State order = 0 < 1 < 2 < ... < K

For this technique to work, the following property must be satisfied: If state j > i, then we cannot encounter state j and then state i afterwards

We define dp[r] = number of objects that reach state i without violating any forbidden state < i for 1 <= i <= K
For each state i, we can compute total(i) = number of ways to reach state i ignoring all constraints

Take any invalid object that reaches state i. Because of monotonicity/ordering, it must violate constraints
for the first time at some state j < i. This j is unique, well-defined and the earliest violation.

This allows a partition of invalid objects: Invalid objects reaching i = union of objects whose first violation is at j, for all j < i
This partition is disjoint.

An object whose first violation is at j has two parts:
1. A valid object from start to j
2. An unconstrained continuation from j to i

Why the first part is valid: because j is the first violation
Therefore, the number of such objects is dp[j] * ways(j -> i)

This is the key multiplicative structure.

Putting everything together:

dp[i] = total(i)
for all j < i: dp[i] -= dp[j] * ways(j -> i)

This is the final transition.

28. Ordered IE DP
Suppose we have some states a, s1, s2, ..., sk < b that are distinct
Suppose we are to find the number of paths from state a to state b such that we don't pass through any of
the states s1, ..., sk.

This can be done using IE but that is exponential in k. This can be reduced to O(k^2) using the following technique.

This technique works if there is an ordering among the states a < s1 < ... < sk < b such that we cannot visit a state j
first and then visit some state i < j

Let 1 <= r <= k
Let dp[r] = number of paths from state a to state sr that do not pass through any state si where i < r
Now dp[r] = total_paths(a, sr) - (paths from state a to state sr that pass through some state si where i < r)

Let a path from state a to state sr be called invalid if it passes through some state si where i < r
Thus, dp[r] = total_paths(a, sr) - (number of invalid paths from state a to state sr)

Let p is an invalid path from state a to state sr. Due to the above property, the path p has a unique
transition state st for 1 <= t <= r - 1 such that p does not pass through any state sk where k < t and p
passes through state st.

Let Ak = set of all paths that have their transition state st = k
Now |Ak| = dp[k] * total_paths(st, sr)

Now, any invalid path from state a to state sr has a transition state st for 1 <= t <= r - 1. Thus
the set of all such invalid paths = Union of Ak for 1 <= k <= r - 1

Also, any Ai and Aj for i < j are disjoint since for any path p1 in Ai and p2 in Aj, p1 and p2 are different
since p1 passes through si and p2 cannot pass through si since j > i.

Thus, the number of such invalid paths = summation of |Ak| for 1 <= k <= r - 1

Thus, dp[r] = total_ways(a, sr) - summation of (dp[k] * total_ways(sk, sr)) for 1 <= k <= r - 1, for 1 <= r <= k
For dp[b], just replace sr in the above formula by b

29. Reverse Inner Loop for DP Space Optimization

This technique applies to DP problems with the following structure:
You process items one by one, and for each item you update DP states that represent a resource limit, i.e, 0/1 knapsack-style DP

Canonical 2D DP formulation
Let:
i = number of items considered
s = resource used

Define dp[i][s] = best/possible using first i items with resource s
Transition: dp[i][s] = either dp[i - 1][s] or dp[i - 1][s - wi] + vi (choose max/OR/min depending on the problem)

Here, dp[i][*] depends only on dp[i - 1][*]

Why space optimization is possible?
Since row i depends only on row i-1, storing the entire table is wasteful

Space reduction options:
1. Use two arrays (prev, curr)
2. Use one array and overwrite carefully

This technique focuses on the second option

If you write dp[s] = max(dp[s], dp[s - w) you are:
1. reading from dp
2. writing to dp

So you must guarantee that dp[s - w] still represents the previous item, not the current one
If this is violated -> same item reused many times

Why forward iteration fails?
Forward Loop (WRONG for 0/1 problems)

for (int s = w; s <= S; s++)
    dp[s] = max(dp[s], dp[s - w])

What happens?
1. dp[s - w] may have already been updated using the current item
2. This allows multiple uses of the same item

Result: You accidentally implement unbounded knapsack

The reverse loop solution (key idea)
for (int s = S; s >= w; s--)
    dp[s] = max(dp[s], dp[s - w])

Since w >= 1, s - w <= s and thus uses results from the last item stored in [1, s - 1], which aren't
altered when we calculate dp[s] when iterating backwards. This is why the reverse loop works and is correct.

Why this works:
When iterating backwards:
1. s - w < s
2. Since we are going from high -> low, dp[s - w] is untouched in this iteration
3. So it still equals dp[i - 1][s - w]

General Structure

0/1 Knapsack DP template (1D)

initialize dp[0, S]
for each item (wt w, value v):
    for s = S down to w:
        dp[s] = combine(dp[s], dp[s - w], v)

Where combine = max, min, or, +, etc

Multidimensional Generalization:

If DP is: dp[i][a][b], you may compress the dimension i, but all resource dimensions must be iterated backwards
for a = A down to wa:
    for b = B down to wb:
        dp[a][b] = ...

If you compress a DP dimension and each choice is allowed only once, iterate resource dimensions backwards.
In 1D optimization, we iterate capacity in reverse to ensure each item is used at most once. Forward iteration would allow the same item to be picked multiple times in a single row.

30. Value-Based Knapsack

When one dimension of DP is too large, check if you can swap it with the answer.
Standard Knapsack: capacity as index, value as answer
Value-based Knapsack: value as index, weight as answer (then filer by capacity)

This swap works when the "large" dimension is what you'd normally index by, and the "small" dimension
(total value) can become the new index. Look at the constraints carefully; they often hint at which formulation
to use

31. If the dp transition looks something like:

initialize dp[0][0]
initialize dp[0][1...W]
initialize dp[1...n][0]
for (int r = 1; r <= n; r++)
    for (int v = 1; v <= W; v++) {
        dp[r][v] = dp[r - 1][v];
        if (v >= v[r]) dp[r][v] = combine(dp[r - 1][v], dp[r - 1][v - v[r]]);
    }

where v[r] >= 1 for 1 <= r <= n, then, it can be space-optimized to a 1D DP as:

initialize dp[0] (corresponds to previous dp[0][0])
initialize dp[1...W] (corresponds to previous dp[0][1..W])
for (int r = 1; r <= n; r++)
    for (int v = W; v >= v[r]; v--)
        dp[v] = combine(dp[v], dp[v - v[r]])

Since v[r] >= 1 and we are iterating backwards, dp[v] would contain dp[r - 1][v] before the updating and dp[v - v[r]]
would contain dp[r - 1][v - v[r]] since we are iterating backwards and wouldn't have updated dp[v - v[r] yet.

32. If the DP transition looks something like:

initialize dp[0][0]
initialize dp[0][1...W]
initialize dp[1...n][0]
for (int r = 1; r <= n; r++)
    for (int v = 1; v <= W; v++) {
        dp[r][v] = dp[r][v];
        if (v >= v[r])
            dp[r][v] = combine(dp[r][v], dp[r][v - v[r]]);
    }

where v[r] >= 1 for 1 <= r <= n, then, it can be space-optimized to a 1D DP as:

initialize dp[0]        (corresponds to dp[0][0])
initialize dp[1...W]    (corresponds to dp[0][1...W])
for (int r = 1; r <= n; r++)
    for (int v = v[r]; v <= W; v++)
        dp[v] = combine(dp[v], dp[v - v[r]])

Since v[r] >= 1 and we iterate forwards, dp[v] would contain dp[r - 1][v] before the updating, and,
dp[v - v[r]] would be dp[r - 1][v - v[r]] since we would've already updated it (since we are iterating forward).

33. Some DP Lessons:

1. If a problem says "subset of a subset" or "choice inside a choice", both choices must be modeled together in a single DP state.
2. If inner decisions depend on outer decisions, add dimensions.
3. Each item may have multiple logical roles:
i. unused
ii. used in the outer subset
iii. used in both outer and inner subset
4. If dp[r] depends only on dp[r - 1], remove r (state compression)
5. When choices are nested and constrained, model them together, define DP precisely, respect bounds strictly, and compress whenever history is irrelevant.

34. Sliding Range Minimum

Let n, k >= 1
Suppose you have a sequence of values v[1, n]

Suppose, at each step i, starting from 1 and reaching till k, you want

min { v[j] + f(j) | L(i) <= j <= R(i)}
, where, 
1. R(i) increases by exactly 1 as i increases by 1
2. L(i) never decreases as i increases
3. f(j) is a function of j only and does not depend on i

This is the sliding window minimum problem
We solve this using a monotonic deque that maintains the following invariants during each step:

1. The deque stores all the indices which are inside the current window
2. The deque is sorted (in non-decreasing order) from front to the back according
   to the value of (v + f) at the index in the deque

This ensures that the front of the deque contains the index of the minimum of (v + f) inside the current window

deque<int> dq;
for (int i = 1; i <= k; i++) {  
    int right = R(i);
    int left = L(i);

    // insert first
    while (!dq.empty() && (v[dq.back()] + f(dq.back())) >= v[right] + f(right))
        dq.pop_back();
    dq.push_back(right);

    // delete expired elements later
    while (!dq.empty() && dq.front() < left)
        dq.pop_front();

    if (!dq.empty()) {
        int mIdx = dq.front();
        int mVal = v[mIdx] + f(mIdx); // minimum of the window at step i 
    }
}

The updation at each step works in amortized O(1), and the querying at each step works in O(1)

35. If we have some jobs j[1, n], and they have
some corresponding deadlines d[1, n], then, any optimal order of completing them will be increasing order of their deadlines.

36. Whenever a problem says “at most X”, model the DP as “exactly X” and sum at the end.

37. = 0 v/s >= 1 DP Optimization

This optimization applies in counting DPs where a variable represents a non-negative integer number of times something is used (repetitions allowed) and the transition loops over that count (e.g. for v = 0...).

Instead of enumerating all v, split into two exhaustive and mutually exclusive cases: v = 0 (do not use it -> transition from the previous layer, e.g., dp[i - 1][state]) and v >= 1 (use it once and reduce the problem -> transition from the same layer with reduced state, e.g, dp[i][state - cost]).

This works because all solutions with v >= 1 can be generated by repeatedly
addint one, so, the current DP accumulates them.

When to apply: counting ways, repetition allowed, cost changes as a function of the number of times v is used, and you see a loop over "how many times".

How to apply: replace the v loop with dp[i] = dp[i - 1] (v = 0) + dp[i] shifted by one cost (v >= 1).

If a DP loops over how many times something is used, split into 0 and ≥ 1 and reuse the DP.

Never loop over “how many times” in counting DP—split into 0 and ≥1 and reuse the DP.

38. Counting Knapsack

1. Combinations v/s Permutations is controlled by loop order. If the coins are the outer loop, then we are counting combinations; if the amount is the outer loop, then we are counting permutations.

2. Forward v/s backward inner loop depends on bounded v/s unbounded. Bounded (use each item once) needs backward. Unbounded (unlimited use) can go forward.

39. Bounded Knapsack

Problem: Given items (wi, vi, ki) and capacity W, maximize total value
with at most ki copies of each item.

Core Idea: Bounded knapsack is reduced to 0/1 knapsack using binary decomposition of counts.

Binary Decomposition Trick:

Any count k can be written as a sum of powers of two (i.e., its binary representation).

So, replace item (w, v, k) with multiple items:

(w * (1 << b), v * (1 << b))

for each set bit 0 <= b < 64 in k.

This converts bounded knapsack to 0/1 knapsack with O(sum(log(kr); 1 <= r <= n)) items.

Why it works?

Binary decomposition ensures:
1. Every number 0, ..., k is representable.
2. Each decomposed item is used at most once.
3. Total choice remains exact.

The binary decomposition is the key. Instead of k copies of each item, you create O(logk) bundles. Any count from 1 to k can be formed by combining these bundles. This makes nded knapsack nearly as fast as 0/1.

Complexity:

1. Time: O(W * sum(log(kr); 1 <= r <= n))
2. Space: O(W)

Pattern Recognition:

1. If problem says "at most k times", we have bounded knapsack
2. If k is large, think of binary splitting.
3. If k is small and many items, think of monotonic queue DP

40. Greedy before DP

If a DP involves choosing items where feasibility of choosing item r depends on previous chosen items, and there is no fixed natural order (like time, index, or position), then the order itself is part of the problem.

In such cases, first look for a greedy ordering that preserves feasibility (usually proven via an exchange argument), and only then apply DP in that order.

The greedy step does not choose items; it only fixes an order so that local DP transitions remain globally valid.

Typical signals include: constraints like "item r can only be chosen if previous total <= limit(r)", objects that add cost but have capacity, or rules where choosing later items can invalidate earlier ones.

Common orderings minimize future harm (e.g., sort by cost + capacity, deadlines, or slack). After this ordering, DP becomes straightforward and safe.

If DP validity depends on history/future, ask: "Is the order wrong?" -> Fix order greedily, then DP.

To recognize this pattern:
1. DP state depends of previous/future aggregates (weight, time, load, sum)
2. Items are symmetric initially (no given order)
3. Picking an item early can break feasibility later
4. Greedy alone fails, DP alone fails -> Greedy order + DP works

ORDER FIXES FEASIBILITY while DP OPTIMIZES VALUE

When DP feasibility depends on history and items are unordered, first prove a greedy order is WLOG using an exchange argument on two items; then DP over than order.

To justify restricting the DP to a specific order, it suffices to show that for any two items i and j (i > j) that appear consecutively in a feasible solution but violate the desired order (K(i) > K(j)), swapping them preserves feasibility and does not decrease the objective value.

By repeatedly applying such swaps, any optimal solution can be transformed into one respecting the order, hence the restriction is without loss of generality.

In order-sensitive DP, prove a greedy order is WLOG via an exchange argument: show that any inversion can be swapped without harming feasibility or value; then DP over that order is valid.

41. Linear/Near-Linear Counting of Prefix Comparisons with Bounded Transitions

We are given a sequence (string/array) and asked to count substrings (or index pairs) satisfying a condition that can be rewritten as:

p[j] > p[i] for 0 <= i < j <= N

where p[k] is a prefix derived quantity.

General Solution: Fenwick Trees/Merge Sort which is O(NlogN)

This technique reduces the time complexity to O(N * K), where K is a small constant, if the prefix transitions are bounded.

Critical Assumption (When This Works)

Assume that the prefix function satisfies p[k] - p[k - 1] belongs to [a, b] where

1. a and b are constants
2. b - a = K is small and independent of n

Under this assumption:
1. Prefix values are bounded by O(N)
2. Between consecutive positions, p only moves a small distance on the number line

Maintained State (Invariants)
We process indices in increasing order j = 1 -> N

Variables:
1. d = p[j] -> current prefix value
2. cnt[x] -> number of indices i < j with p[i] = x
3. sum = number of indices i < j such that p[i] < d
4. ans = total number of valid pairs counted so far

Core Invariant
At the start of iteration j:

sum = # {i < j : p[i] < p[j - 1]}
This invariant is the foundation of the algorithm

Updating the Invariant

Let:
1. old = p[j - 1]
2. new = p[j]
3. delta = new - old, where a <= delta <= b

We must update sum so that it becomes

sum = # {i < j : p[i] < new}

1. Case 1: Prefix Increases (delta > 0)

New prefix values that become strictly smaller than new: old, old + 1, ..., new - 1

Update:
for x = old to new - 1:
    sum += cnt[x]

2. Case 2: Prefix Decreases (delta < 0)
Prefix values that are no longer strictly smaller: new, new + 1, ..., old - 1

Update:
for x = new to old - 1
    sum -= cnt[x]

3. Case 3: Prefix Unchanged (delta = 0)
Nothing changes: sum unchanged

Counting Valid pairs
After updating sum:

# {i < j : p[i] < p[j]} = sum

So we do: ans += sum
Then register the current prefix for future steps: cnt[p[j]]++

Correctness Argument
1. Each valid pair (i, j) is counted exactly once, at iteration j.
2. The invariant guarantees that sum is always correct.
3. Only prefix values that cross the comparison boundary are adjusted.

If prefix values change only by a small bounded amount each step, the count of smaller previous prefixes can be maintained incrementally in O(1) or O(K) time per step, yielding an O(N) or O(N·K) solution.

42. DP Optimization via Modulo Decomposition + Sliding Window

We consider DP transitions of the form

dp[i][x] = OP (0 <= t <= ki) (dp[i - 1][x - t * wi] + f(i, t)) for 1 <= i <= n, 0 <= x <= S

where:

1. i -> stage/item index
2. x -> DP state (weight, sum, position)
3. wi -> fixed step size
4. ki -> upper bound on steps
5. f(i, t) -> additive cost depending only on t
6. OP is {sum, max, or min}

Naive Complexity

For each (i, x) we try all t: O(N * S * K)
This is infeasible when k is large

Structural Observation (The Core Invariant)

All predecessor states used to compute dp[i][x] are dp[i - 1][x], dp[i - 1][x - wi], dp[i - 1][x - 2 * wi], ...

These indicies:
1. form an AP
2. share the same remainder modulo wi

This enables the entire optimization.

Modulo Decomposition

Fix an item i
For each remainder i in {0, 1, ..., wi - 1} consider only the states:

x = r + t * wi for (S - r) / wi >= t >= 0

Define a reduced sequence:

A[r][t] = dp[i - 1][r + t * wi]

Substituting this into the original transition, we get:

dp[i][r + t * wi] = OP (t - ki <= j <= t) (A[r][j] + f(i, t - j)) for 1 <= i <= n, for 0 <= r < wi, for 0 <= t <= (S - r) / wi

For a fixed i and r, the DP is now 1D over t
As t increases by 1, the valid range of j shifts from [t - ki, t] to [t + 1 - ki, t + 1], i.e, a sliding window of fixed width

Hence, the reduced DP is a sliding window aggregate over a linear sequence.

We show an example:

Let the transition be:
dp[i][x] = max(dp[i - 1][x - t * wi] + t * vi, for 0 <= t <= ki) for 1 <= i <= n, 0 <= x <= S

For some 1 <= i <= n, let x = r + j * wi for some 0 <= r < wi and 0 <= j <= (S - r) / wi
Then, 

dp[i][r + j * wi] = max(dp[i - 1][r + (j - t) * wi] - (j - t) * vi + j * vi, for 0 <= t <= ki) for 1 <= i <= n, 0 <= r < wi, and 0 <= j <= (S - r) / wi

i.e,

dp[i][r + j * wi] = max(dp[i - 1][r + t * wi] - t * wi + j * vi, for max(0, j - ki) <= t <= j)) for 1 <= i <= n, 0 <= r < wi, and 0 <= j <= (S - r) / wi

for (int i = 1; i <= n; i++)
    for (int r = 0; r < wi; r++)
        for (int j = 0; j <= (S - r) / wi; j++) {
            int left = max(0, j - ki);
            int right = j;

            dp[i][r + j * wi] = LLONG_MIN;
            for (int t = left; t <= right; t++)
                dp[i][r + j * wi] = max(dp[i][r + j * wi], dp[i - 1][r + t * wi] - t * wi) + j * vi;
        }

The innermost transition is now a sliding window maximum and thus can be optimized using a deque

for (int i = 1; i <= n; i++)
    for (int r = 0; r < wi; r++) {
        deque<int> d;
        for (int j = 0; j <= (S - r) / wi; j++) {
            int left = max(0, j - ki);
            int right = j;

            while (!d.empty() && dp[i - 1][r + d.back() * wi] - d.back() * vi > dp[i - 1][r + right * wi] - right * vi)
                d.pop_back();
            d.push_back(right);

            while (!d.empty() && d.front() < left)
                d.pop_front();

            dp[i][r + j * wi] = LLONG_MIN;
            if (!d.empty()) dp[i][r + j * wi] = dp[i - 1][r + d.front() * wi] - d.front() * vi + j * vi;
        }
    }

43. In some dynamic programming problems, although the total number of elements or transitions is very large, the number of distinct element types is small enough to satisfy the constraints; in such cases, the DP can be significantly simplified by grouping identical elements and reformulating the transition as a bounded (0 / k) knapsack. 

The key observation is that if multiple elements contribute identically to the DP state—having the same weight, cost, or effect—then only their count matters, not their individual identities or order. 

By counting the frequency of each distinct type and treating each type as a single item with weight w, value v, and multiplicity k, the original DP over many items reduces to transitions of the form dp[x] = (dp[x − t * w]+t * v). 

This compression reduces the effective problem size from the total number of elements to the number of unique types and enables the use of standard bounded-knapsack optimizations such as binary decomposition, monotonic queue optimization, or modulo + sliding-window DP. 

Conceptually, this technique exploits the fact that a large input with a small “alphabet” can be processed by frequency aggregation, turning an otherwise infeasible DP into a tractable one.

44. DP via Exchange Argument

i. Problem Pattern
This technique applies when:
1. The items are initially unordered
2. The DP is order-sensitive (state depends on processed prefix)
3. Feasibility or value depends on history, not just set membership
4. Goal is to optimize or test feasibility over all permutations

ii. Core Principle
If an optimal solution can always be transformed into one that follows a specific order, then DP may be restricted to that order without loss of generality (WLOG).

The order is justified using a pairwise exchange argument.

iii. Step 1: Deriving the Order (Greedy Justification)
Define a pairwise preference rule between two items i, j
Consider two consecutive items in a feasible/optimal solution:

... -> i -> j -> ...
Suppose the desired order requires j before i

To justify this, prove:
Swapping i and j preserves feasibility and does not decrease objective value.

Formally:
If K(i) > K(j), then value(j -> i) >= value (i -> j) and feasibility is unchanged.

This establishes that j before i is safe.

iv. Step 2: Exchange Argument
1. Any solution violating the order has at least one adjacent inversion
2. Each inversion can be swapped safely
3. Repeating swaps eliminates all inversions
4. Hence, an optimal solution exists that respects the order

Therefore, restricting the DP to this order is WLOG.

v. Step 3: DP on the fixed order
Once sorted:
1. Process items sequentially
2. DP state depends only on prefix
3. No need to consider other permutations

vi. Comparator Construction Rule

The comparator encodes the pairwise exchange argument in executable form. A comparator must answer: "If I must process exactly two items back-to-back, which order is worse?"

Given two items i and j, define
1. C(i -> j): cost/feasibility effect of processing i then j
2. C(j -> i): cost/feasibility effect of processing j then i

Then, the comparator must conclude that i < j iff C(i -> j) < C(j -> i). If the cost values are equal, the order doesn't matter.

Why Two-Item Analysis is Sufficient?
Exchange arguments rely on locality:
1. Any permutation can be converted into another by adjacent swaps
2. If every local inversion is safe to swap, then global optimality follows
3. Therefore, comparing only two consecutive items is enough

This is why the comparator must be derived from a pairwise isolated scenario.

The comparator must be strict:

i < j iff C(i -> j) < C(j -> i)

Never use <=
Equal values mean order doesn't matter

The comparator must induce a strict weak ordering on the set of items. This means that the comparator defines weak order (i < j) and is transitive.

45. Bracket Sequence 

Let s is a string consisting of only the characters '(' and ')' of length n.

1. s is a correct bracket sequence iff all the '(' characters in the string can be paired with a ')' character in the string occuring after it.

2. Let f(x) = +1 if s[x] = '(' 
            = -1 otherwise

Then, s is a correct bracket sequence iff

1. summation (f(x); 1 <= x <= n) = 0
2. summation (f(x); 1 <= x <= r) >= 0 for all 1 <= r <= n

3. Let s is not a correct bracket sequence.
Let r(x) = summation(f(i); 1 <= i <= x) for 1 <= x <= n

Let g = min(r(x) for 1 <= x <= n)

Then, the optimal number of characters to be removed from s to make it a correct bracket sequence is (r(n) + 2 * |g|).

Out of these, we remove |g| number of ')' characters from s (the place of removal doesn't matter, so, possibly we remove the first |g| ')' characters), and then, remove r(n) + |g| number of '(' characters from the end of the string s.

46. Even in greedy problems where all items must be taken but the order matters, we can apply an idea similar to the “DP with sorting” technique. By comparing two items at a time—asking which order of taking them leads to a better result, just like in a DP transition—we can derive a pairwise comparator. Sorting all items using this comparator then gives an order that is globally optimal, allowing us to solve the problem greedily after this comparison-based reasoning.

46. Distance Minimizing Toolkit (1D)

1. Sum of Absolute Distances -> Median

Let n >= 1
Let a[1..n] be an array of integers

Define f(x) as:
f(x) = summation of |a[r] - x| for all r from 1 to n, where x is an integer

Let y be the median of the array a[1..n].

Then:
f(y) <= f(x) for any integer x.

If n is even, y may be chosen as any value between the two middle elements
(in particular, either of the two medians).

2. Sum of Weighted Absolute Distances -> Weighted Median

Let n >= 1
Let a[1..n] be an array of integers
Let w[1..n] be an array of positive weights

Define f(x) as:
f(x) = summation of w[r] * |a[r] - x| for all r from 1 to n, where x is an integer

Let W be the summation of all weights w[1..n].

Let y be a weighted median of a[1..n], defined as a value y such that:
    the summation of w[r] for all r with a[r] < y is at most W / 2, and
    the summation of w[r] for all r with a[r] > y is at most W / 2.

Then:
f(y) <= f(x) for any integer x.

If multiple such y exist, any of them minimizes f(x).

47. 

Let f(x) = number of consecutive ones in the binary representation of x, starting from the LSB for x >= 0

i. The number of bits that differ in n and n + 1 (when padded to the same length) = f(n) + 1 for n >= 0

ii. f(x) = largest value of a >= 0 such that (x % 2^a = 2^a - 1)

iii. To count the number of integers x in the range [0, n] (for n >= 0) such that f(x) = a, for some a >= 0, we do the following:

Let this count be y.
Now, y = number of integers x in the range [0, n] such that (y % 2^a = 2^a - 1 && y % 2^(a + 1) != 2^(a + 1) - 1)

The integers x in the range [0, n] such that x % b = b - 1 form the following AP:

b - 1, 2 * b - 1, ....,

where b = 2^a and this AP has g1 = (n - b) / b + 1 elements

The integers x in the range [0, n] such that x % d = d - 1 form the following AP:

d - 1, 2 * d - 1, ...,

where d = 2^(a + 1) and this AP has g2 = (n - d) / d + 1

Some rth term of the first AP and qth term of the second AP are equal iff r = 2 * q

Let g = (n - b) / b + 1
Then, it follows that the number of terms common in both the APs is exactly floor(g1 / 2). 

Thus y = g1 - floor(g1 / 2)  

48. n ^ (n + 1) ^ (n + 2) = n + 3 for any positive integer n

49. 

Let a[1, n] be an array of non-negative integers and let p[1, n] be its prefix XOR array.

Then,

1. XOR of subsegment a[l, r] = p[r] ^ p[l - 1] for any 1 <= l <= r <= n

2. a[r] = p[r] ^ p[r - 1] for any 1 <= r <= n

50. Consider the following loop

int cnt = 0;
for (int r = 1; r <= n; r++)
    for (int i = 1; i < r; i++)
        if (arr[i] >= arr[r])
            cnt++;

This is equivalent to counting the number of inversions (i < j and arr[i] >= arr[j]) in the array and can be done easily using the merge sort algorithm in O(nlog(n)) time.

int cnt = 0;
void merge(int l, int r, vector<int> &arr) {
    if (l == r) return;

    int sz = r - l + 1;
    merge(l, l + sz / 2 - 1, arr);
    merge(l + sz / 2, r, arr);

    vector<int> tmp(sz + 1);
    int i = l, j = l + sz / 2, k = 1;
    while (i <= l + sz / 2 - 1 && j <= r)
        if (arr[i] < arr[j]) tmp[k++] = arr[i++];
        else cnt += ((l + sz / 2) - i), tmp[k++] = arr[j++];

    while (i <= l + sz / 2 - 1)
        tmp[k++] = arr[i++];
    while (j <= r)
        tmp[k++] = arr[j++];

    for (int q = l; q <= r; q++)
        arr[q] = tmp[q - l + 1];
}

51. Greedy Dominance Principle

Greedy dominance is a principle where among multiple states representing the same "progress", some states are never better than others for any future transition.

Such states are called dominated and can be discarded safely.

Suppose a DP has states of the form dp[state]

A state A dominates state B if:
1. A and B represent the same stage (same length, same step)
2. For every possible future transition, A performs at least as well as B

Then, state B can be deleted without affecting the optimal answer.

i. Why dominance enables Greedy Algorithms

DP usually fails because there are too many states. Greedy dominance allows you to:

1. Keep only non-dominated states
2. Permanently discard inferior states
3. Reduce state count from quadratic/exponential to linear/logarithmic

This is how DP turns into Greedy + Binary Search

ii. LIS as an example:

Original DP:
dp[i] = LIS ending at i

Compressed State:
last[k] = smallest ending value of any LIS of length k

Dominance Rule:

Same length k -> smaller ending value dominates larger ending value

Hence, only one state per length is needed.

iii. The General Pattern

Greedy Dominance works when all the following holds:

1. States are comparable
There exists a total or partial order on states.

2. Future transitions depend on few parameters
The future only depends on current state parameters and not the entire history.

3. Monotonicity of transitions
If state A is better than state B now, then A remains better after any future step.

4. Exchange Argument Exists
You can prove: Any optimal solution using B can be transformed into one using A without reducing optimality.

iv. How to use this technique

1. Write the DP

dp[i][j] = best result after i steps with parameter j

2. Identify the "progress dimension"
Find what defines the stage:
i. length
ii. time
iii. number of items used
iv. number of operations

In LIS, it's length.

3. Find dominance within the same stage

Ask: Among all states within the same stage, which ones are always better.

4. Keep only minimal/maximal representative

Replace a set of states by a single best state per stage.

4. Maintain order and search

If the best states are ordered:
1. Use binary search
2. Use two pointers
3. Use monotonic queue/convex hull

General Template:

1. Define DP states.
2. Identify equivalent stages.
3. Define dominance relation.
4. Prove dominance via exchange argument.
5. Discard dominated states greedily.
6. Optimize transitions with data structure.

Greedy dominance works when you can totally order states so that “better now” implies “better forever”.

Binary Search LIS works exactly when DP states of equal length have a total dominance order by ending values.

We can use the binary search LIS method iff:

For increasing subsequences of the same length, the one with the smallest ending value dominates all others.

Formally, if two subsequences have equal length k and end at x <= y, then the one ending at x is always at least as extendable as the one ending at y. If this dominance holds, we can use the nlogn LIS. Otherwise, we need to resort to dp + segment/fenwick trees.

52. LIS

Longest Strictly Increasing Subsequence

int32_t main() {
    int n;
    cin >> n;

    vector<int> arr(n + 1), last(n + 2);
    for (int r = 1; r <= n; r++) cin >> arr[r];

    last[0] = LLONG_MIN;
    for (int r = 1; r <= n + 1; r++)
        last[r] = LLONG_MAX;

    // last[r] = smallest element that ends a strictly increasing
    // subsequence of length r for >= 1

    // Claim: last is always sorted in strictly increasing order
    // Proof: Suppose last[i] >= last[i + 1]
    // This means that there is a strictly increasing subsequence of length i + 1
    // that ends at last[i + 1]. The ith element of this subsequence (say x)
    // will be < last[i + 1]. Thus, there is a strictly increasing subsequence
    // of length i that ends at x < last[i], which is a contradiction.

    for (int r = 1; r <= n; r++) {
        int left = 0;
        int right = n + 1;

        // find the largest i such that last[i] < arr[r] for 0 <= i <= n
        while (right != left + 1) {
            int mid = left + (right - left) / 2;
            if (last[mid] < arr[r])
                left = mid;
            else 
                right = mid;
        }

        last[left + 1] = arr[r];
    }

    for (int r = n; r >= 1; r--)
        if (last[r] != LLONG_MAX) {
            cout << r << " ";
            break;
        }
    return 0;
}   

53. Number of Longest (Strictly)Increasing Subsequence

#include <bits/stdc++.h>
using namespace std;

#define int long long

int32_t main() {
    int n;
    cin >> n;

    vector<int> arr(n + 1);
    for (int r = 1; r <= n; r++) cin >> arr[r];

    // dp[r] is the length of the longest increasing subsequence
    // ending at r for 1 <= r <= n;

    // cnt[r] is the number of subsequences ending at r with length 
    // dp[r]

    int maxi = LLONG_MIN;
    int res = 0;
    vector<int> dp(n + 1), cnt(n + 1);
    for (int r = 1; r <= n; r++) {
        dp[r] = 1;
        cnt[r] = 1;

        for (int i = 1; i < r; i++)
            if (arr[i] < arr[r]) {
                if (dp[i] + 1 > dp[r]) dp[r] = dp[i] + 1, cnt[r] = cnt[i];
                else if (dp[i] + 1 == dp[r]) cnt[r] += cnt[i];
            }
        
        if (dp[r] > maxi) res = cnt[r], maxi = dp[r];
        else if (dp[r] == maxi) res += cnt[r];
    }

    cout << res << endl;
    return 0;
}

You track both the length and count of LIS ending at each position. When you find a longer subsequence, reset the count. When you find an equal-length one, add to the count.

54. 2D Longest Increasing Subsequence

1. Problem Statement

Given envelopes E = {(wr, hr)},

envelope i fits into envelope j iff wi < wj and hi < hj

Goal: find the maximum length envelope chain a1, a2, ... such that

a1 fits into a2, a2 fits into a3, ..., and so on

2. Naive DP

Define: dp[i] = max chain ending at envelope i

Transition
dp[i] = 1
for (int r = 1; r < i; r++)
    if (w[r] < w[i] && h[r] < h[i])
        dp[i] = max(dp[i], dp[r] + 1);

Time O(n^2)
Correct but slow.

3. Goal of Optimization

Reduce 2D partial order to a 1D LIS problem, which can be solved in O(n * log(n))

To do this, we must:

i. eliminate one dimension via sorting
ii. ensure the remaining dimension fully encodes validity

5. Sorting strategy

We sort envelopes by:
i. width increasing
ii. height decreasing if widhts are equal

6. Why sorting is required?

After sorting by width:

i < j -> wi <= wj

Thus,

i. width constraint is implicitly handled
ii. LIS only needs to check heights

But, this introduces a danger when wi = wj

7. Invariant required for LIS correctness

When we run LIS on heights, we assume:

if i < j and hi < hj, then envelope i can nest into j
This assumption is false when wi = wj

Thus, sorting must prevent height-increasing pairs with equal width.

8. What breaks if height is NOT sorted descending

Counterexample:

(5,4), (5,7)

widths equal → nesting impossible
but heights increase → LIS counts both

Result: invalid chain

9. Why height descending fixes it
For equal widths, sorting height descending gives:

(5, 7), (5, 4)

Heights are decreasing, so:
i. LIS can pick at most one
ii. Equal width envelopes never form a chain

10. Key invariant after sorting

After sorting by (width increasing, height decreasing):

If i < j and hi < hj, then necessarily wi < wj
This guarantees that every increasing height subsequence corresponds to a valid nesting chain.

11. Reduction to LIS

After sorting, define sequence:

H = {h1, h2..., hn}

Now solve: LIS of H
This gives the maximum nesting depth.

In general, when we want to reduce the dimensions of a LIS to a single dimensional LIS on some dimension x, we need to ensure the following invariant:

that every increasing x subsequence corresponds to a valid chain

55. Suppose we have an array a[1, n] for some n >= 1 and we want to find the minimum number of deletions required to sort the array in strictly increasing order.

To do this, find the length longest strictly increasing subsequence, say its x.

Then, the answer is n - x.

56. If you store a tree in an adjacency list format and do a DFS on it using the following signature

int dfs(int node, int parent, vector<vector<int>> &tree);

and the dfs is called from some root x as:

dfs(x, -1, tree)

then, the correct way to check whether the current node is a leaf node is:

(tree[node].size() == 1 && parent != -1)

A node is a leaf node iff the size of its adjacency list is 1 AND it's NOT the root node.
 
57. 
Let n >= 1
Let S = {1, 2, ..., n}

1. For any 1 <= r <= (n * (n + 1)) / 2, there exists a subset s of S such that sum(s) = r.

2. Let 1 <= k <= n
Let s1 = k(k + 1) / 2 (sum of the smallest k integers in S).
Let s2 = k * (2n - k + 1) / 2 (sum of the largest k integers in S)

Then, for any s1 <= r <= s2, there is a subset s of S of size k such that sum(s) = r.

To find this set s for sum r, we use the following greedy construction:

1. Start from the minimum set

A = {1, 2, ..., k}
d = r - (k * (k + 1)) / 2

2. Greedy right-to-left shifting

Write A = {a1 < a2 < ... < ak}, a[i] = i for 1 <= i <= k

For i = k, k-1, ... ,1:
    di = min(d, n - k + i - a[i])
    set ai = a[i] + di, d = d - di
    stop when d = 0

3. We will finally have a subset s of S of size k that sums to r.